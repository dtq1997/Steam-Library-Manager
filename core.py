import base64
import json
import os
import re
import secrets
import shutil
import ssl
import time
import urllib.error
import urllib.request
from datetime import datetime
from json import JSONDecodeError
from tkinter import messagebox

from account_manager import SteamAccount
from local_storage import BackupManager


class NoRedirectHandler(urllib.request.HTTPRedirectHandler):
    """ç¦æ­¢è‡ªåŠ¨é‡å®šå‘ï¼Œä»¥ä¾¿æ£€æµ‹ 302 ç­‰é‡å®šå‘å“åº”"""
    def redirect_request(self, req, fp, code, msg, headers, newurl):
        return None  # è¿”å› None è¡¨ç¤ºä¸è·Ÿéšé‡å®šå‘


class SteamToolboxCore:
    """æ ¸å¿ƒç±»ï¼ŒUI æ— å…³"""

    def __init__(self, account: SteamAccount):
        self.current_account: SteamAccount = account  # å½“å‰é€‰ä¸­çš„è´¦å·

        # è¿™äº›å±æ€§ä¼šåœ¨é€‰æ‹©è´¦å·åè®¾ç½®
        self.backup_manager = BackupManager(self.current_account.storage_path)  # å¤‡ä»½ç®¡ç†å™¨

        # æ•°æ®ç›®å½•ï¼ˆç»Ÿä¸€å­˜æ”¾é…ç½®å’Œç¼“å­˜ï¼‰
        self.data_dir = os.path.join(os.path.expanduser("~"), ".steam_toolbox")
        os.makedirs(self.data_dir, exist_ok=True)
        self.global_config_path = os.path.join(self.data_dir, "config.json")

        # è¿ç§»æ—§ç‰ˆæ–‡ä»¶ï¼ˆä»ä¸»ç›®å½•æ•£è½æ–‡ä»¶ â†’ ç»Ÿä¸€ç›®å½•ï¼‰
        self.migrate_old_files()

        self.induce_suffix = "(åˆ é™¤è¿™æ®µå­—ä»¥è§¦å‘äº‘åŒæ­¥)"
        self.disclaimer = f"\n\n(è‹¥å…¶ä¸­åŒ…å«æœªæ‹¥æœ‰çš„æ¸¸æˆã€é‡å¤æ¡ç›®æˆ–æ˜¯ DLCï¼Œä¼šå¯¼è‡´ Steam æ”¶è—å¤¹å†…æ˜¾ç¤ºçš„æ•°ç›®åå°‘ã€‚)"

        # SSL ä¸Šä¸‹æ–‡ï¼ˆè§£å†³ macOS è¯ä¹¦é—®é¢˜ï¼‰
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE


    def migrate_old_files(self):
        """å°†æ—§ç‰ˆæ•£è½åœ¨ä¸»ç›®å½•çš„æ–‡ä»¶è¿ç§»åˆ°ç»Ÿä¸€æ•°æ®ç›®å½•"""
        home = os.path.expanduser("~")
        migrations = [
            (".steam_toolbox_config.json", "config.json"),
            (".steam_toolbox_igdb_cache.json", "igdb_cache.json"),
        ]
        for old_name, new_name in migrations:
            old_path = os.path.join(home, old_name)
            new_path = os.path.join(self.data_dir, new_name)
            if os.path.exists(old_path) and not os.path.exists(new_path):
                try:
                    shutil.move(old_path, new_path)
                except:
                    pass

    @staticmethod
    def next_version(data):
        """æ‰«æå…¨éƒ¨æ¡ç›®ï¼Œè¿”å›ä¸‹ä¸€ä¸ªå¯ç”¨çš„å…¨å±€ç‰ˆæœ¬å·ï¼ˆå­—ç¬¦ä¸²ï¼‰"""
        max_ver = 0
        for entry in data:
            try:
                v = int(entry[1].get("version", "0"))
                if v > max_ver: max_ver = v
            except (ValueError, IndexError, TypeError):
                continue
        return str(max_ver + 1)

    def add_static_collection(self, data, name, app_ids):
        col_id = f"uc-{secrets.token_hex(6)}"
        storage_key = f"user-collections.{col_id}"
        val_obj = {"id": col_id, "name": name + self.induce_suffix, "added": app_ids, "removed": []}
        new_entry = [storage_key, {"key": storage_key, "timestamp": int(time.time()),
                                   "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                                   "version": self.next_version(data),
                                   "conflictResolutionMethod": "custom", "strMethodId": "union-collections"}]
        data.append(new_entry)

    def load_config(self):
        """åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.global_config_path):
            try:
                with open(self.global_config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_config(self, config):
        """ä¿å­˜å…¨å±€é…ç½®æ–‡ä»¶"""
        try:
            with open(self.global_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except:
            pass

    def get_saved_cookie(self):
        """è·å–å·²ä¿å­˜çš„ Cookieï¼ˆç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        encoded = config.get("steam_cookie_encoded", "")
        if encoded:
            try:
                return base64.b64decode(encoded.encode()).decode()
            except:
                pass
        return ""

    def save_cookie(self, cookie_value):
        """ä¿å­˜ Cookieï¼ˆç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        if cookie_value:
            config["steam_cookie_encoded"] = base64.b64encode(cookie_value.encode()).decode()
        else:
            config.pop("steam_cookie_encoded", None)
        self.save_config(config)

    def clear_saved_cookie(self):
        """æ¸…é™¤å·²ä¿å­˜çš„ Cookie"""
        config = self.load_config()
        config.pop("steam_cookie_encoded", None)
        self.save_config(config)

    # ==================== IGDB API ç›¸å…³å‡½æ•° ====================
    def get_igdb_credentials(self):
        """è·å–å·²ä¿å­˜çš„ IGDB API å‡­è¯"""
        config = self.load_config()
        client_id = config.get("igdb_client_id", "")
        encoded_secret = config.get("igdb_client_secret_encoded", "")
        client_secret = ""
        if encoded_secret:
            try:
                client_secret = base64.b64decode(encoded_secret.encode()).decode()
            except:
                pass
        return client_id, client_secret

    def save_igdb_credentials(self, client_id, client_secret):
        """ä¿å­˜ IGDB API å‡­è¯ï¼ˆClient Secret ç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        config["igdb_client_id"] = client_id
        if client_secret:
            config["igdb_client_secret_encoded"] = base64.b64encode(client_secret.encode()).decode()
        else:
            config.pop("igdb_client_secret_encoded", None)
        self.save_config(config)

    def clear_igdb_credentials(self):
        """æ¸…é™¤ IGDB API å‡­è¯"""
        config = self.load_config()
        config.pop("igdb_client_id", None)
        config.pop("igdb_client_secret_encoded", None)
        config.pop("igdb_access_token", None)
        config.pop("igdb_token_expires_at", None)
        self.save_config(config)

    def get_igdb_access_token(self, force_refresh=False):
        """è·å– IGDB API çš„è®¿é—®ä»¤ç‰Œï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        client_id, client_secret = self.get_igdb_credentials()
        if not client_id or not client_secret:
            return None, "æœªé…ç½® IGDB API å‡­è¯"

        config = self.load_config()
        cached_token = config.get("igdb_access_token", "")
        expires_at = config.get("igdb_token_expires_at", 0)

        # æ£€æŸ¥ç¼“å­˜çš„ä»¤ç‰Œæ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆæå‰ 300 ç§’è¿‡æœŸï¼‰
        current_time = int(time.time())
        if not force_refresh and cached_token and expires_at > current_time + 300:
            return cached_token, None

        # è¯·æ±‚æ–°çš„è®¿é—®ä»¤ç‰Œ
        token_url = f"https://id.twitch.tv/oauth2/token?client_id={client_id}&client_secret={client_secret}&grant_type=client_credentials"

        try:
            req = urllib.request.Request(token_url, method='POST')
            with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as resp:
                data = json.loads(resp.read().decode('utf-8'))

            access_token = data.get("access_token", "")
            expires_in = data.get("expires_in", 0)

            if not access_token:
                return None, "è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥ï¼šå“åº”ä¸­æ—  access_token"

            # ç¼“å­˜ä»¤ç‰Œ
            config["igdb_access_token"] = access_token
            config["igdb_token_expires_at"] = current_time + expires_in
            self.save_config(config)

            return access_token, None

        except urllib.error.HTTPError as e:
            return None, f"HTTP é”™è¯¯ {e.code}ï¼šè·å– IGDB ä»¤ç‰Œå¤±è´¥"
        except urllib.error.URLError as e:
            return None, f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return None, f"è·å–ä»¤ç‰Œå¤±è´¥ï¼š{str(e)}"

    def fetch_igdb_genres(self, progress_callback=None):
        """è·å– IGDB æ¸¸æˆç±»å‹åˆ—è¡¨"""
        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()

        if error:
            return [], error

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è·å–æ¸¸æˆç±»å‹åˆ—è¡¨...", "")

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        # è·å–æ‰€æœ‰æ¸¸æˆç±»å‹
        url = "https://api.igdb.com/v4/genres"
        body = "fields id,name,slug; limit 100;"

        try:
            req = urllib.request.Request(url, data=body.encode('utf-8'), headers=headers, method='POST')
            with urllib.request.urlopen(req, timeout=20, context=self.ssl_context) as resp:
                genres = json.loads(resp.read().decode('utf-8'))

            # æŒ‰åç§°æ’åº
            genres.sort(key=lambda x: x.get('name', ''))
            return genres, None

        except urllib.error.HTTPError as e:
            return [], f"HTTP é”™è¯¯ {e.code}ï¼šè·å–ç±»å‹åˆ—è¡¨å¤±è´¥"
        except urllib.error.URLError as e:
            return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return [], f"è·å–å¤±è´¥ï¼š{str(e)}"

    # ==================== IGDB æœ¬åœ°ç¼“å­˜ ====================

    IGDB_CACHE_EXPIRY_DAYS = 7  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰

    def get_igdb_cache_path(self):
        """è·å– IGDB ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.data_dir, "igdb_cache.json")

    def load_igdb_cache(self):
        """åŠ è½½ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_igdb_cache(self, cache):
        """ä¿å­˜ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False)
        except:
            pass

    def get_igdb_genre_cache(self, genre_id):
        """è·å–æŸä¸ªç±»å‹çš„ç¼“å­˜æ•°æ®ï¼Œè¿”å› (steam_ids, cached_at_timestamp) æˆ– (None, None)"""
        cache = self.load_igdb_cache()
        genre_key = str(genre_id)
        if genre_key in cache:
            entry = cache[genre_key]
            return entry.get("steam_ids", []), entry.get("cached_at", 0)
        return None, None

    def set_igdb_genre_cache(self, genre_id, steam_ids):
        """å†™å…¥æŸä¸ªç±»å‹çš„ç¼“å­˜æ•°æ®"""
        cache = self.load_igdb_cache()
        cache[str(genre_id)] = {
            "steam_ids": steam_ids,
            "cached_at": time.time(),
        }
        self.save_igdb_cache(cache)

    def is_igdb_cache_valid(self, cached_at):
        """åˆ¤æ–­ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        if not cached_at:
            return False
        age_seconds = time.time() - cached_at
        return age_seconds < self.IGDB_CACHE_EXPIRY_DAYS * 86400

    def get_igdb_cache_summary(self):
        """è·å–ç¼“å­˜æ‘˜è¦ä¿¡æ¯ï¼Œç”¨äº UI æ˜¾ç¤º

        Returns:
            dict: {'total_genres': int, 'total_games': int, 'oldest_at': float, 'newest_at': float,
                   'is_full_dump': bool, 'total_steam_games': int}
                  å¦‚æœæ— ç¼“å­˜åˆ™è¿”å› None
        """
        cache = self.load_igdb_cache()
        if not cache:
            return None

        meta = cache.get("_meta", {})
        is_full_dump = meta.get("type") == "full_dump"

        # ç»Ÿè®¡æ—¶æ’é™¤ _meta é”®
        genre_entries = {k: v for k, v in cache.items() if k != "_meta" and isinstance(v, dict)}
        if not genre_entries:
            return None

        total_genres = len(genre_entries)
        total_games = sum(len(entry.get("steam_ids", [])) for entry in genre_entries.values())
        timestamps = [entry.get("cached_at", 0) for entry in genre_entries.values() if entry.get("cached_at")]
        if not timestamps:
            return None
        return {
            'total_genres': total_genres,
            'total_games': total_games,
            'oldest_at': min(timestamps),
            'newest_at': max(timestamps),
            'is_full_dump': is_full_dump,
            'total_steam_games': meta.get("total_steam_games", 0),
        }

    def clear_igdb_genre_cache(self):
        """æ¸…é™¤æ‰€æœ‰ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        if os.path.exists(path):
            try:
                os.remove(path)
            except:
                pass

    # ==================== IGDB API è¯·æ±‚ ====================

    def igdb_api_request(self, url, body, headers):
        """å‘é€ IGDB API è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†é€Ÿç‡é™åˆ¶å’Œé‡è¯•"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                req = urllib.request.Request(url, data=body.encode('utf-8'), headers=headers, method='POST')
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    return json.loads(resp.read().decode('utf-8')), None
            except urllib.error.HTTPError as e:
                if e.code == 429:
                    time.sleep(1.5)
                    continue
                return None, f"HTTP é”™è¯¯ {e.code}"
            except urllib.error.URLError as e:
                return None, f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
            except Exception as e:
                return None, f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
        return None, "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰"

    def build_igdb_full_cache(self, progress_callback=None, cancel_flag=None):
        """ä¸‹è½½ IGDB ä¸­æ‰€æœ‰æœ‰ Steam å…³è”çš„æ¸¸æˆåŠå…¶ç±»å‹ä¿¡æ¯ï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜ã€‚

        ç­–ç•¥ï¼šå…ˆä» external_games æ‹‰å–æ‰€æœ‰ Steam å…³è”ï¼Œå†æ‰¹é‡æŸ¥ genresã€‚

        Args:
            progress_callback: fn(current, total, phase_str, detail_str)
                               current/total ç”¨äºé©±åŠ¨è¿›åº¦æ¡ï¼ˆtotal>0 è¡¨ç¤ºå·²çŸ¥æ€»é‡ï¼‰
            cancel_flag: list[bool]ï¼Œcancel_flag[0]=True æ—¶ä¸­æ­¢

        Returns:
            (genre_map, error): genre_map = {genre_id: [steam_app_ids]}, error = str | None
        """
        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return {}, error

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        # ===== é¢„æŸ¥è¯¢ï¼šè·å– Steam å…³è”è®°å½•çš„æœ€å¤§ IDï¼Œç”¨äºä¼°ç®—è¿›åº¦ =====
        # external_game_source = 1 å³ Steamï¼ˆæ—§å­—æ®µ category å·²è¢« IGDB åºŸå¼ƒï¼Œå…¨éƒ¨ä¸º nullï¼‰
        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨ä¼°ç®—æ•°æ®é‡...", "")

        max_ext_id = 0
        body = "fields id; where external_game_source = 1; sort id desc; limit 1;"
        results, err = self.igdb_api_request(
            "https://api.igdb.com/v4/external_games", body, headers)
        if results:
            max_ext_id = results[0].get('id', 0)
        time.sleep(0.28)

        # ===== ç¬¬1æ­¥ï¼šéå† external_games è·å–æ‰€æœ‰ Steam å…³è” =====
        # igdb_game_id â†’ steam_app_id
        game_to_steam = {}
        last_id = 0
        limit = 500

        while True:
            if cancel_flag and cancel_flag[0]:
                return {}, "ç”¨æˆ·å–æ¶ˆ"

            if progress_callback:
                # ç”¨ last_id / max_ext_id ä¼°ç®—ç¬¬1æ­¥è¿›åº¦ï¼ˆå æ€»ä½“ 50%ï¼‰
                step1_pct = (last_id / max_ext_id * 50) if max_ext_id > 0 else 0
                progress_callback(int(step1_pct), 100,
                                  "æ­£åœ¨ä¸‹è½½ Steam æ¸¸æˆåˆ—è¡¨...",
                                  f"å·²è·å– {len(game_to_steam)} ä¸ªæ¸¸æˆ")

            body = (f"fields id,uid,game; "
                    f"where external_game_source = 1 & id > {last_id}; "
                    f"sort id asc; limit {limit};")

            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/external_games", body, headers)

            if err:
                return {}, f"ä¸‹è½½ Steam æ¸¸æˆåˆ—è¡¨å¤±è´¥ï¼š{err}"
            if not results:
                break

            for item in results:
                uid = item.get('uid', '')
                game_id = item.get('game')
                ext_id = item.get('id', 0)
                if uid and uid.isdigit() and game_id:
                    game_to_steam[int(game_id)] = int(uid)
                if ext_id > last_id:
                    last_id = ext_id

            if len(results) < limit:
                break
            time.sleep(0.28)

        if not game_to_steam:
            return {}, "æœªæ‰¾åˆ°ä»»ä½• Steam æ¸¸æˆ"

        # ===== ç¬¬2æ­¥ï¼šæ‰¹é‡æŸ¥è¯¢è¿™äº›æ¸¸æˆçš„ genres =====
        all_game_ids = list(game_to_steam.keys())
        genre_map = {}  # genre_id â†’ set of steam_app_ids
        batch_size = 500
        total_batches = (len(all_game_ids) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            if cancel_flag and cancel_flag[0]:
                return {}, "ç”¨æˆ·å–æ¶ˆ"

            if progress_callback:
                # ç¬¬2æ­¥å æ€»ä½“ 50%~100%
                step2_pct = 50 + (batch_idx / total_batches * 50) if total_batches > 0 else 50
                progress_callback(int(step2_pct), 100,
                                  "æ­£åœ¨ä¸‹è½½æ¸¸æˆåˆ†ç±»ä¿¡æ¯...",
                                  f"è¿›åº¦ {batch_idx + 1}/{total_batches}ï¼ˆå…± {len(all_game_ids)} ä¸ªæ¸¸æˆï¼‰")

            batch = all_game_ids[batch_idx * batch_size: (batch_idx + 1) * batch_size]
            ids_str = ",".join(str(gid) for gid in batch)

            body = (f"fields id,genres; "
                    f"where id = ({ids_str}); "
                    f"limit {limit};")

            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/games", body, headers)

            if err:
                time.sleep(0.28)
                continue

            if results:
                for item in results:
                    gid = item.get('id')
                    genres = item.get('genres', [])
                    if gid and gid in game_to_steam:
                        steam_id = game_to_steam[gid]
                        for genre_id in genres:
                            genre_map.setdefault(genre_id, set()).add(steam_id)

            time.sleep(0.28)

        # ===== ç¬¬3æ­¥ï¼šå†™å…¥ç¼“å­˜ =====
        cache = {}
        now = time.time()
        for genre_id, steam_ids_set in genre_map.items():
            cache[str(genre_id)] = {
                "steam_ids": sorted(steam_ids_set),
                "cached_at": now,
            }
        cache["_meta"] = {
            "type": "full_dump",
            "cached_at": now,
            "total_steam_games": len(game_to_steam),
            "total_genres": len(genre_map),
        }
        self.save_igdb_cache(cache)

        if progress_callback:
            progress_callback(100, 100,
                              "âœ… ä¸‹è½½å®Œæˆ",
                              f"å…± {len(game_to_steam)} ä¸ª Steam æ¸¸æˆï¼Œè¦†ç›– {len(genre_map)} ä¸ªç±»å‹")

        return {gid: sorted(sids) for gid, sids in genre_map.items()}, None

    def fetch_igdb_games_by_genre(self, genre_id, genre_name, progress_callback=None, force_refresh=False):
        """æ ¹æ®ç±»å‹ ID è·å–è¯¥ç±»å‹ä¸‹æ‰€æœ‰æ¸¸æˆçš„ Steam AppID

        ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å…¨é‡ç¼“å­˜ã€‚å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œåˆ™è‡ªåŠ¨è§¦å‘å…¨é‡æ„å»ºã€‚
        """
        if not force_refresh:
            # å…ˆæ£€æŸ¥è¯¥ç±»å‹æ˜¯å¦æœ‰ç¼“å­˜
            cached_ids, cached_at = self.get_igdb_genre_cache(genre_id)
            if cached_ids is not None and self.is_igdb_cache_valid(cached_at):
                if progress_callback:
                    age_hours = (time.time() - cached_at) / 3600
                    progress_callback(len(cached_ids), len(cached_ids),
                                      f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜",
                                      f"{genre_name}: {len(cached_ids)} ä¸ªæ¸¸æˆï¼ˆç¼“å­˜äº {age_hours:.0f} å°æ—¶å‰ï¼‰")
                return cached_ids, None

            # è¯¥ç±»å‹æ— ç¼“å­˜ï¼Œä½†å…¨é‡ç¼“å­˜å¯èƒ½å·²æ„å»ºï¼ˆåªæ˜¯è¯¥ç±»å‹ç¡®å®æ²¡æœ‰ Steam æ¸¸æˆï¼‰
            cache = self.load_igdb_cache()
            meta = cache.get("_meta", {})
            if meta.get("type") == "full_dump" and self.is_igdb_cache_valid(meta.get("cached_at", 0)):
                # å…¨é‡ç¼“å­˜æœ‰æ•ˆï¼Œè¯¥ç±»å‹ç¡®å®æ— æ•°æ®
                if progress_callback:
                    age_hours = (time.time() - meta["cached_at"]) / 3600
                    progress_callback(0, 0,
                                      f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜", f"{genre_name}: 0 ä¸ª Steam æ¸¸æˆï¼ˆç¼“å­˜äº {age_hours:.0f} å°æ—¶å‰ï¼‰")
                return [], None

        # === ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼šè§¦å‘ä¸‹è½½ ===
        if progress_callback:
            progress_callback(0, 0, "æœ¬åœ°æ•°æ®ä¸å®Œæ•´ï¼Œæ­£åœ¨ä» IGDB ä¸‹è½½...", "é¦–æ¬¡ä¸‹è½½çº¦éœ€ 5-8 åˆ†é’Ÿ")

        genre_map, error = self.build_igdb_full_cache(progress_callback)
        if error:
            return [], error

        # ä»åˆšæ„å»ºçš„ç¼“å­˜ä¸­è¿”å›ç»“æœ
        steam_ids = genre_map.get(genre_id, [])
        return steam_ids, None

    def load_json(self):
        if not self.current_account.storage_path or not os.path.exists(self.current_account.storage_path):
            messagebox.showerror("é”™è¯¯", f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²é€‰æ‹©æœ‰æ•ˆçš„ Steam è´¦å·ã€‚")
            return None
        try:
            with open(self.current_account.storage_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            messagebox.showerror("è¯»å–é”™è¯¯", f"è§£æå¤±è´¥: {e}")
            return None

    def save_json(self, data, create_backup=True, backup_description=""):
        """ä¿å­˜ JSON æ•°æ®åˆ°åŸæ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            create_backup: æ˜¯å¦åœ¨ä¿å­˜å‰åˆ›å»ºå¤‡ä»½
            backup_description: å¤‡ä»½æè¿°
        """
        if not self.current_account.storage_path:
            messagebox.showerror("é”™è¯¯", "æœªé€‰æ‹©è´¦å·ï¼Œæ— æ³•ä¿å­˜ã€‚")
            return False

        # åˆ›å»ºå¤‡ä»½
        if create_backup and self.backup_manager:
            backup_path = self.backup_manager.create_backup(description=backup_description)
            if backup_path:
                backup_info = f"\n\nå·²è‡ªåŠ¨å¤‡ä»½è‡³:\n{os.path.basename(backup_path)}"
            else:
                backup_info = "\n\nâš ï¸ å¤‡ä»½åˆ›å»ºå¤±è´¥"
        else:
            backup_info = ""

        # å†™å…¥åŸæ–‡ä»¶ï¼ˆä½¿ç”¨åŸå­å†™å…¥ï¼‰
        tmp_path = self.current_account.storage_path + ".tmp"
        try:
            with open(tmp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, separators=(',', ':'))

            # åŸå­æ›¿æ¢
            if os.path.exists(self.current_account.storage_path):
                os.replace(tmp_path, self.current_account.storage_path)
            else:
                os.rename(tmp_path, self.current_account.storage_path)

            messagebox.showinfo("æˆåŠŸ", f"æ–‡ä»¶å·²ä¿å­˜ï¼š\n{os.path.basename(self.current_account.storage_path)}{backup_info}")
            return True
        except Exception as e:
            messagebox.showerror("ä¿å­˜å¤±è´¥", f"æ— æ³•å†™å…¥æ–‡ä»¶: {e}")
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except:
                    pass
            return False


    @staticmethod
    def sanitize_filename(name):
        """æ¸…æ´—æ–‡ä»¶åï¼Œæ›¿æ¢ç³»ç»Ÿç¦æ­¢çš„ç‰¹æ®Šå­—ç¬¦"""
        return re.sub(r'[\\/*?:"<>|]', '_', name).strip()

    def get_static_collections(self, data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        return self.get_all_collections_with_refs(data)

    @staticmethod
    def get_all_collections_with_refs(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€æ”¶è—å¤¹ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    icon = "ğŸ”" if is_dynamic else "ğŸ“"
                    collections.append({
                        "entry_ref": entry,
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name"),
                        "added": val_obj.get("added", []),
                        "is_dynamic": is_dynamic,
                        "display_name": f"{icon} {val_obj.get('name', 'æœªå‘½å')}"
                    })
                except Exception:
                    continue
        collections.sort(key=lambda c: (c.get('name') or '').lower())
        return collections

    @staticmethod
    def get_all_collections_ordered(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆæŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¸ Steam å®¢æˆ·ç«¯ä¸€è‡´ï¼‰"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    col_info = {
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name", "æœªå‘½å"),
                        "added": val_obj.get("added", []),
                        "removed": val_obj.get("removed", []),
                        "is_dynamic": is_dynamic
                    }
                    if is_dynamic:
                        col_info["filterSpec"] = val_obj.get("filterSpec")
                    collections.append(col_info)
                except Exception:
                    continue
        collections.sort(key=lambda c: c['name'].lower())
        return collections

    @staticmethod
    def extract_ids_from_html(html_text):
        """æ ¸å¿ƒæå–é€»è¾‘ï¼šä» HTML ä¸­æå– AppID"""
        search_area = html_text
        list_start = html_text.find('id="RecommendationsRows"')
        if list_start == -1:
            list_start = html_text.find('class="creator_grid_ctn"')

        if list_start != -1:
            footer_start = html_text.find('id="footer"', list_start)
            search_area = html_text[list_start: (footer_start if footer_start != -1 else len(html_text))]

        raw_matches = re.findall(r'data-ds-appid="([\d,]+)"', search_area)
        all_ids = []
        for m in raw_matches:
            if ',' in m:
                all_ids.extend(m.split(','))
            else:
                all_ids.append(m)

        return list(dict.fromkeys([int(aid) for aid in all_ids if aid.isdigit()]))

    def extract_page_name_from_html(self, html_text, url_hint=""):
        """ä» HTML ä¸­æ™ºèƒ½æå–é¡µé¢åç§°ï¼ˆå¸¦ç±»å‹å‰ç¼€ï¼‰"""
        type_name_cn = "åˆ—è¡¨"
        if url_hint:
            page_type, _ = self.extract_steam_list_info(url_hint)
            type_names = {
                "curator": "é‰´èµå®¶",
                "publisher": "å‘è¡Œå•†",
                "developer": "å¼€å‘å•†",
                "franchise": "ç³»åˆ—",
                "genre": "ç±»å‹",
                "category": "åˆ†ç±»",
            }
            type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        if "curator" in html_text.lower() or "é‰´èµå®¶" in html_text:
            type_name_cn = "é‰´èµå®¶"
        elif "publisher" in html_text.lower():
            type_name_cn = "å‘è¡Œå•†"
        elif "developer" in html_text.lower():
            type_name_cn = "å¼€å‘å•†"

        name = None
        match = re.search(r'class="curator_name".*?><a.*?>(.*?)</a>', html_text, re.S)
        if match:
            name = match.group(1).strip()

        if not name:
            match = re.search(r'<title>(.*?)</title>', html_text, re.I)
            if match:
                title = match.group(1)
                title = re.sub(r'\s*[-â€“â€”]\s*Steam.*$', '', title, flags=re.I)
                title = re.sub(r'\s*on Steam.*$', '', title, flags=re.I)
                title = re.sub(r'^Steam é‰´èµå®¶ï¼š', '', title)
                title = re.sub(r'^Steam Curator:\s*', '', title, flags=re.I)
                name = title.strip()

        if name:
            return f"{type_name_cn}ï¼š{name}"
        return f"{type_name_cn}ï¼šæœªçŸ¥"

    def extract_curator_name(self, html_text):
        """ä» HTML ä¸­æ™ºèƒ½æå–é‰´èµå®¶åç§°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self.extract_page_name_from_html(html_text)

    @staticmethod
    def extract_steam_list_info(url_or_id):
        """ä» URL æˆ–ç›´æ¥è¾“å…¥ä¸­æå– Steam åˆ—è¡¨é¡µé¢ä¿¡æ¯"""
        text = url_or_id.strip()

        if text.isdigit():
            return "curator", text

        patterns = [
            (r'/curator/(\d+)', "curator"),
            (r'/publisher/([^/?#]+)', "publisher"),
            (r'/developer/([^/?#]+)', "developer"),
            (r'/franchise/([^/?#]+)', "franchise"),
            (r'/genre/([^/?#]+)', "genre"),
            (r'/category/([^/?#]+)', "category"),
        ]

        for pattern, page_type in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return page_type, match.group(1)

        return None, None

    def fetch_steam_list(self, page_type, identifier, progress_callback=None, login_cookies=None):
        """é€šè¿‡ Steam API è‡ªåŠ¨è·å–åˆ—è¡¨é¡µé¢çš„æ‰€æœ‰æ¸¸æˆ"""
        type_names = {
            "curator": "é‰´èµå®¶",
            "publisher": "å‘è¡Œå•†",
            "developer": "å¼€å‘å•†",
            "franchise": "ç³»åˆ—",
            "genre": "ç±»å‹",
            "category": "åˆ†ç±»",
        }
        type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        base_cookies = "birthtime=283993201; wants_mature_content=1; mature_content=1; lastagecheckage=1-0-1979; steamCountry=US%7C0"
        has_login = login_cookies is not None and len(login_cookies.strip()) > 0

        if has_login:
            cookies = f"{login_cookies}; {base_cookies}"
        else:
            cookies = base_cookies

        if page_type in ("curator", "publisher", "developer"):
            return self.fetch_curator_style_api(page_type, identifier, type_name_cn, cookies, has_login,
                                                 progress_callback)
        else:
            return self.fetch_generic_list(page_type, identifier, type_name_cn, cookies, has_login, progress_callback)

    def fetch_curator_style_api(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """ç»Ÿä¸€çš„ ajaxgetfilteredrecommendations API æŠ“å–"""
        from urllib.parse import unquote

        page_url = f"https://store.steampowered.com/{page_type}/{identifier}/"
        curator_id = None
        page_name = None

        headers_html = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cookie': cookies,
        }

        if page_type == "curator":
            curator_id = identifier
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨éªŒè¯é‰´èµå®¶é¡µé¢...", "æ­£åœ¨è¿æ¥ Steam å•†åº—...")
            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')

                name_patterns = [
                    r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                    r'<title>Steam é‰´èµå®¶ï¼š([^<]+?)</title>',
                    r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
                ]
                for pattern in name_patterns:
                    match = re.search(pattern, html_content, re.S | re.I)
                    if match:
                        extracted = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                        extracted = extracted.replace('&amp;', '&').replace('&quot;', '"')
                        if extracted and len(extracted) < 100:
                            page_name = extracted
                            break

            except urllib.error.HTTPError:
                pass
            except Exception:
                pass
        else:
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢ä¿¡æ¯...", f"æ­£åœ¨è®¿é—® {page_type}/{identifier} ...")

            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')

                clanid_match = re.search(r'curator_clanid[=:][\s"\']*(\d+)', html_content)
                if clanid_match:
                    curator_id = clanid_match.group(1)

                name_patterns = [
                    r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                    r'<title>(?:Steam (?:Publisher|Developer):\s*)?([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
                ]
                for pattern in name_patterns:
                    match = re.search(pattern, html_content, re.S | re.I)
                    if match:
                        extracted = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                        extracted = extracted.replace('&amp;', '&').replace('&quot;', '"')
                        if extracted and len(extracted) < 100:
                            page_name = extracted
                            break

            except urllib.error.HTTPError as e:
                return [], None, f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—®è¯¥{type_name_cn}é¡µé¢ã€‚", has_login
            except Exception as e:
                return [], None, f"è·å–é¡µé¢å¤±è´¥ï¼š{str(e)}", has_login

        if not curator_id:
            return [], None, f"æ— æ³•ä»è¯¥{type_name_cn}é¡µé¢æå– curator IDã€‚", has_login

        base_url = f"https://store.steampowered.com/curator/{curator_id}/ajaxgetfilteredrecommendations/"

        lang_configs = [
            ("schinese", "zh-CN,zh;q=0.9,en;q=0.8", "ç®€ä½“ä¸­æ–‡"),
            ("english", "en-US,en;q=0.9", "English"),
            ("japanese", "ja,en;q=0.8", "æ—¥æœ¬èª"),
            ("tchinese", "zh-TW,zh;q=0.9,en;q=0.8", "ç¹é«”ä¸­æ–‡"),
            ("koreana", "ko,en;q=0.8", "í•œêµ­ì–´"),
        ]

        all_unique_ids = set()
        max_total = 0

        for lang_idx, (lang_code, accept_lang, lang_display) in enumerate(lang_configs):
            headers_api = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': accept_lang,
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': page_url,
                'Cookie': cookies,
            }

            start = 0
            count = 100
            total_count = None
            lang_page = 0

            if progress_callback:
                progress_callback(
                    len(all_unique_ids), max_total,
                    f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                    f"ğŸŒ æ‰«æè¯­è¨€ [{lang_idx + 1}/{len(lang_configs)}]ï¼š{lang_display} â€” æ­£åœ¨è¿æ¥..."
                )

            while True:
                url = f"{base_url}?start={start}&count={count}&l={lang_code}"
                lang_page += 1

                try:
                    req = urllib.request.Request(url, headers=headers_api)
                    with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                        data = json.loads(resp.read().decode('utf-8'))

                    if not data.get('success'):
                        break

                    if total_count is None:
                        total_count = int(data.get('total_count', 0))
                        if total_count == 0:
                            break
                        if total_count > max_total:
                            max_total = total_count

                    html_chunk = data.get('results_html', '')
                    new_in_page = 0
                    if html_chunk:
                        chunk_ids = re.findall(r'data-ds-appid="(\d+)"', html_chunk)
                        for aid in chunk_ids:
                            aid_int = int(aid)
                            if aid_int not in all_unique_ids:
                                new_in_page += 1
                            all_unique_ids.add(aid_int)

                        if page_name is None:
                            name_match = re.search(r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>', html_chunk, re.S)
                            if name_match:
                                page_name = re.sub(r'<[^>]+>', '', name_match.group(1)).strip()

                    if progress_callback:
                        total_pages = (total_count + count - 1) // count if total_count else "?"
                        progress_callback(
                            len(all_unique_ids), max_total,
                            f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                            f"ğŸŒ [{lang_idx + 1}/{len(lang_configs)}] {lang_display} â€” ç¬¬ {lang_page}/{total_pages} é¡µï¼ˆæœ¬é¡µæ–°å¢ {new_in_page}ï¼Œå…± {len(chunk_ids) if html_chunk else 0} æ¡ï¼‰"
                        )

                    start += count
                    if start >= total_count or not html_chunk:
                        break

                    time.sleep(0.1)

                except Exception:
                    break

            if progress_callback:
                progress_callback(
                    len(all_unique_ids), max_total if max_total else len(all_unique_ids),
                    f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                    f"âœ… {lang_display} æ‰«æå®Œæˆ â€” å½“å‰å…± {len(all_unique_ids)} ä¸ªå”¯ä¸€æ¸¸æˆ"
                )

            time.sleep(0.2)

        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}æ²¡æœ‰ä»»ä½•æ¸¸æˆï¼Œæˆ–æ ‡è¯†ç¬¦æ— æ•ˆã€‚\nè¯·æ£€æŸ¥ URL æ˜¯å¦æ­£ç¡®ã€‚", has_login

        unique_ids = list(all_unique_ids)

        if page_name:
            display_name = f"{type_name_cn}ï¼š{page_name}"
        else:
            display_name = f"{type_name_cn}ï¼š{unquote(identifier)}"

        return unique_ids, display_name, None, has_login

    def fetch_generic_list(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """é€šè¿‡é€šç”¨æ–¹å¼æŠ“å–å‘è¡Œå•†/å¼€å‘å•†/ç³»åˆ—ç­‰é¡µé¢çš„æ¸¸æˆåˆ—è¡¨"""
        from urllib.parse import unquote

        base_url = f"https://store.steampowered.com/{page_type}/{identifier}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cookie': cookies,
        }

        all_unique_ids = set()
        page_name = None

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢...", f"æ­£åœ¨è¿æ¥ {page_type}/{identifier} ...")

        try:
            req = urllib.request.Request(base_url, headers=headers)
            with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            name_patterns = [
                r'<div class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                r'<div class="page_title_area[^"]*"[^>]*>.*?<span[^>]*>(.*?)</span>',
                r'<h2 class="pageheader">(.*?)</h2>',
                r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam|\s*on Steam)?</title>',
            ]

            for pattern in name_patterns:
                match = re.search(pattern, html_content, re.S | re.I)
                if match:
                    extracted_name = match.group(1).strip()
                    extracted_name = re.sub(r'<[^>]+>', '', extracted_name)
                    extracted_name = extracted_name.replace('&amp;', '&').replace('&quot;', '"')
                    if extracted_name and len(extracted_name) < 100:
                        page_name = extracted_name
                        break

            if not page_name:
                page_name = unquote(identifier).replace('%20', ' ').replace('+', ' ')

            ids = self.extract_ids_from_html(html_content)
            for aid in ids:
                all_unique_ids.add(aid)

            if progress_callback:
                progress_callback(len(all_unique_ids), len(all_unique_ids), "å·²è·å–ä¸»é¡µé¢",
                                  f"ğŸ“„ ä¸»é¡µé¢æå–äº† {len(ids)} ä¸ªæ¸¸æˆï¼Œæ­£åœ¨æ£€æŸ¥åˆ†é¡µ...")

            page = 2
            while True:
                ajax_url = f"{base_url}?page={page}"
                try:
                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"æ­£åœ¨è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ æ­£åœ¨åŠ è½½ç¬¬ {page} é¡µ...")

                    req = urllib.request.Request(ajax_url, headers=headers)
                    with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as resp:
                        page_html = resp.read().decode('utf-8')

                    page_ids = self.extract_ids_from_html(page_html)
                    if not page_ids or all(aid in all_unique_ids for aid in page_ids):
                        break

                    new_count = sum(1 for aid in page_ids if aid not in all_unique_ids)
                    for aid in page_ids:
                        all_unique_ids.add(aid)

                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"å·²è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ ç¬¬ {page} é¡µæ–°å¢ {new_count} ä¸ªæ¸¸æˆï¼Œå½“å‰å…± {len(all_unique_ids)} ä¸ª")

                    page += 1
                    time.sleep(0.3)

                    if page > 50:
                        break

                except Exception:
                    break

        except urllib.error.HTTPError as e:
            return [], None, f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—®è¯¥é¡µé¢ã€‚", has_login
        except Exception as e:
            return [], None, f"è·å–å¤±è´¥ï¼š{str(e)}", has_login

        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}é¡µé¢æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¸¸æˆã€‚", has_login

        unique_ids = list(all_unique_ids)
        display_name = f"{type_name_cn}ï¼š{page_name}"

        return unique_ids, display_name, None, has_login

    @staticmethod
    def extract_ids_from_steamdb_html(html_text):
        """ä» SteamDB é¡µé¢æºä»£ç ä¸­æå– AppID"""
        tbody_match = re.search(r'<tbody.*?>(.*?)</tbody>', html_text, re.DOTALL)
        if not tbody_match:
            return []
        return [int(aid) for aid in re.findall(r'data-appid="(\d+)"', tbody_match.group(1))]

    def perform_incremental_update(self, data, target_entry, new_ids_from_src, raw_name):
        """æ ¸å¿ƒå¢é‡æ›´æ–°é€»è¾‘ï¼šä¸»æ”¶è—å¤¹è¿½åŠ  + ç”Ÿæˆä¸¤ä¸ªå·®å¼‚å¤‡ä»½æ–‡ä»¶å¤¹

        Returns:
            (added_count, removed_count, total_count, is_updated)
            å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œis_updated ä¸º Falseï¼Œæ­¤æ—¶ä¸ä¼šåšä»»ä½•ä¿®æ”¹
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_ids = val_obj.get("added", [])

        old_set = set(old_ids)
        src_set = set(new_ids_from_src)

        added_list = [aid for aid in new_ids_from_src if aid not in old_set]
        removed_list = [aid for aid in old_ids if aid not in src_set]

        # å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œä¸åšä»»ä½•æ“ä½œ
        if not added_list:
            return 0, len(removed_list), len(old_ids), False

        # æœ‰æ–°å¢ï¼Œæ‰§è¡Œæ›´æ–°
        val_obj['added'] = old_ids + added_list
        clean_name = raw_name.replace(self.induce_suffix, "").strip()
        val_obj['name'] = f"{clean_name}{self.induce_suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        # åˆ›å»ºè¾…åŠ©æ”¶è—å¤¹
        self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå¤šçš„", added_list)
        if removed_list:
            self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå°‘çš„", removed_list)

        return len(added_list), len(removed_list), len(val_obj['added']), True

    def perform_replace_update(self, data, target_entry, new_ids):
        """æ›¿æ¢å¼æ›´æ–°ï¼šç›´æ¥ç”¨æ–° ID åˆ—è¡¨æ›¿æ¢ç›®æ ‡æ”¶è—å¤¹çš„å†…å®¹

        Returns:
            (old_count, new_count)
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_count = len(val_obj.get("added", []))

        val_obj['added'] = new_ids
        clean_name = val_obj.get('name', '').replace(self.induce_suffix, "").strip()
        val_obj['name'] = f"{clean_name}{self.induce_suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        return old_count, len(new_ids)

    # --- æ”¶è—å¤¹å¯¼å‡º/å¯¼å…¥ï¼ˆä¸¤ç§æ ¼å¼ï¼‰ ---

    @staticmethod
    def export_collections_appid_list(collections):
        """æ ¼å¼ä¸€ï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å»é‡ AppID åˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰
        åŠ¨æ€æ”¶è—å¤¹åªå¯¼å‡ºå…¶ added åˆ—è¡¨ã€‚"""
        seen = set()
        unique_ids = []
        for col in collections:
            for aid in col.get('added', []):
                if aid not in seen:
                    seen.add(aid)
                    unique_ids.append(aid)
        return unique_ids

    @staticmethod
    def export_collections_structured(collections):
        """æ ¼å¼äºŒï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å®Œæ•´ç»“æ„åŒ– JSON
        åŒ…å«åç§°ã€ç±»å‹ã€appidã€åŠ¨æ€é€»è¾‘ç­‰ã€‚"""
        export_data = {
            "format": "steam_collections_structured",
            "version": 1,
            "exported_at": datetime.now().isoformat(),
            "collections": []
        }
        for col in collections:
            entry = {
                "name": col.get("name", "æœªå‘½å"),
                "is_dynamic": col.get("is_dynamic", False),
                "added": col.get("added", []),
                "removed": col.get("removed", []),
            }
            if col.get("is_dynamic") and col.get("filterSpec"):
                entry["filterSpec"] = col["filterSpec"]
            export_data["collections"].append(entry)
        return export_data

    def import_collections_appid_list(self, file_path, data):
        """æ ¼å¼ä¸€ï¼šå¯¼å…¥ä¸€è¡Œä¸€ä¸ª AppID çš„åˆ—è¡¨æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªæ–°æ”¶è—å¤¹"""
        file_title = os.path.splitext(os.path.basename(file_path))[0]
        with open(file_path, 'r', encoding='utf-8') as f:
            app_ids = [int(line.strip()) for line in f if line.strip().isdigit()]
        if not app_ids:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ AppIDã€‚"
        self.add_static_collection(data, file_title, app_ids)
        return len(app_ids), None

    def import_collections_structured(self, file_path, data):
        """æ ¼å¼äºŒï¼šå¯¼å…¥ç»“æ„åŒ– JSON æ–‡ä»¶ï¼Œè¿˜åŸå¤šä¸ªæ”¶è—å¤¹ï¼ˆå«åŠ¨æ€é€»è¾‘ï¼‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
        except JSONDecodeError as e:
            return None, "æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"

        if import_data.get("format") != "steam_collections_structured":
            return None, "æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…ï¼šç¼ºå°‘ format æ ‡è¯†ã€‚"

        imported_cols = import_data.get("collections", [])
        if not imported_cols:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æ”¶è—å¤¹æ•°æ®ã€‚"

        count = 0
        for col in imported_cols:
            name = col.get("name", "å¯¼å…¥çš„æ”¶è—å¤¹")
            is_dynamic = col.get("is_dynamic", False)
            added = col.get("added", [])
            removed = col.get("removed", [])

            if is_dynamic and "filterSpec" in col:
                # è¿˜åŸåŠ¨æ€æ”¶è—å¤¹
                col_id = f"uc-{secrets.token_hex(4)}"
                storage_key = f"user-collections.{col_id}"
                val_obj = {
                    "id": col_id,
                    "name": name + self.induce_suffix,
                    "added": added,
                    "removed": removed,
                    "filterSpec": col["filterSpec"]
                }
                new_entry = [storage_key, {
                    "key": storage_key,
                    "timestamp": int(time.time()),
                    "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                    "version": self.next_version(data),
                    "conflictResolutionMethod": "custom",
                    "strMethodId": "union-collections"
                }]
                data.append(new_entry)
            else:
                # é™æ€æ”¶è—å¤¹
                self.add_static_collection(data, name.replace(self.induce_suffix, "").strip(), added)
            count += 1

        return count, None

    def add_dynamic_collection(self, data, name, friend_code):
        col_id = f"uc-{secrets.token_hex(4)}"
        storage_key = f"user-collections.{col_id}"
        filter_groups = [{"rgOptions": [], "bAcceptUnion": False} for _ in range(9)]
        filter_groups[0]["bAcceptUnion"] = True
        filter_groups[6]["rgOptions"] = [int(friend_code)]
        val_obj = {"id": col_id, "name": name + self.induce_suffix, "added": [], "removed": [],
                   "filterSpec": {"nFormatVersion": 2, "strSearchText": "", "filterGroups": filter_groups,
                                  "setSuggestions": {}}}
        new_entry = [storage_key, {"key": storage_key, "timestamp": int(time.time()),
                                   "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                                   "version": self.next_version(data),
                                   "conflictResolutionMethod": "custom", "strMethodId": "union-collections"}]
        data.append(new_entry)

    def fetch_steam250_ids(self, url, progress_callback=None):
        """ä» Steam250 é¡µé¢æå– AppID åˆ—è¡¨"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        }

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è¿æ¥ Steam250...", "")

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=20, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è§£æé¡µé¢...", "")

            raw_ids = re.findall(r'store\.steampowered\.com/app/(\d+)', html_content)

            unique_ids = []
            for aid in raw_ids:
                if aid not in unique_ids:
                    unique_ids.append(aid)

            app_ids = [int(aid) for aid in unique_ids[:250]]

            if not app_ids:
                return [], "æœªèƒ½ä»é¡µé¢æå–åˆ°ä»»ä½• AppIDã€‚é¡µé¢ç»“æ„å¯èƒ½å·²å˜åŒ–ã€‚"

            return app_ids, None

        except urllib.error.HTTPError as e:
            return [], f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—® Steam250ã€‚"
        except urllib.error.URLError as e:
            return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return [], f"æå–å¤±è´¥ï¼š{str(e)}"


