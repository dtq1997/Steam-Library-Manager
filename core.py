import base64
import json
import os
import re
import secrets
import shutil
import ssl
import time
import urllib.error
import urllib.request
from datetime import datetime
from json import JSONDecodeError
from tkinter import messagebox

from account_manager import SteamAccount
from local_storage import BackupManager


class NoRedirectHandler(urllib.request.HTTPRedirectHandler):
    """ç¦æ­¢è‡ªåŠ¨é‡å®šå‘ï¼Œä»¥ä¾¿æ£€æµ‹ 302 ç­‰é‡å®šå‘å“åº”"""
    def redirect_request(self, req, fp, code, msg, headers, newurl):
        return None  # è¿”å› None è¡¨ç¤ºä¸è·Ÿéšé‡å®šå‘


class SteamToolboxCore:
    """æ ¸å¿ƒç±»ï¼ŒUI æ— å…³"""

    def __init__(self, account: SteamAccount):
        self.current_account: SteamAccount = account  # å½“å‰é€‰ä¸­çš„è´¦å·

        # è¿™äº›å±æ€§ä¼šåœ¨é€‰æ‹©è´¦å·åè®¾ç½®
        self.backup_manager = BackupManager(self.current_account.storage_path)  # å¤‡ä»½ç®¡ç†å™¨

        # æ•°æ®ç›®å½•ï¼ˆç»Ÿä¸€å­˜æ”¾é…ç½®å’Œç¼“å­˜ï¼‰
        self.data_dir = os.path.join(os.path.expanduser("~"), ".steam_toolbox")
        os.makedirs(self.data_dir, exist_ok=True)
        self.global_config_path = os.path.join(self.data_dir, "config.json")

        # è¿ç§»æ—§ç‰ˆæ–‡ä»¶ï¼ˆä»ä¸»ç›®å½•æ•£è½æ–‡ä»¶ â†’ ç»Ÿä¸€ç›®å½•ï¼‰
        self.migrate_old_files()

        self.induce_suffix = "(åˆ é™¤è¿™æ®µå­—ä»¥è§¦å‘äº‘åŒæ­¥)"
        self.disclaimer = f"\n\n(è‹¥å…¶ä¸­åŒ…å«æœªæ‹¥æœ‰çš„æ¸¸æˆã€é‡å¤æ¡ç›®æˆ–æ˜¯ DLCï¼Œä¼šå¯¼è‡´ Steam æ”¶è—å¤¹å†…æ˜¾ç¤ºçš„æ•°ç›®åå°‘ã€‚)"

        # SSL ä¸Šä¸‹æ–‡ï¼ˆè§£å†³ macOS è¯ä¹¦é—®é¢˜ï¼‰
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE


    def migrate_old_files(self):
        """å°†æ—§ç‰ˆæ•£è½åœ¨ä¸»ç›®å½•çš„æ–‡ä»¶è¿ç§»åˆ°ç»Ÿä¸€æ•°æ®ç›®å½•"""
        home = os.path.expanduser("~")
        migrations = [
            (".steam_toolbox_config.json", "config.json"),
            (".steam_toolbox_igdb_cache.json", "igdb_cache.json"),
        ]
        for old_name, new_name in migrations:
            old_path = os.path.join(home, old_name)
            new_path = os.path.join(self.data_dir, new_name)
            if os.path.exists(old_path) and not os.path.exists(new_path):
                try:
                    shutil.move(old_path, new_path)
                except:
                    pass

    @staticmethod
    def next_version(data):
        """æ‰«æå…¨éƒ¨æ¡ç›®ï¼Œè¿”å›ä¸‹ä¸€ä¸ªå¯ç”¨çš„å…¨å±€ç‰ˆæœ¬å·ï¼ˆå­—ç¬¦ä¸²ï¼‰"""
        max_ver = 0
        for entry in data:
            try:
                v = int(entry[1].get("version", "0"))
                if v > max_ver: max_ver = v
            except (ValueError, IndexError, TypeError):
                continue
        return str(max_ver + 1)

    def add_static_collection(self, data, name, app_ids):
        col_id = f"uc-{secrets.token_hex(6)}"
        storage_key = f"user-collections.{col_id}"
        val_obj = {"id": col_id, "name": name + self.induce_suffix, "added": app_ids, "removed": []}
        new_entry = [storage_key, {"key": storage_key, "timestamp": int(time.time()),
                                   "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                                   "version": self.next_version(data),
                                   "conflictResolutionMethod": "custom", "strMethodId": "union-collections"}]
        data.append(new_entry)

    def load_config(self):
        """åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.global_config_path):
            try:
                with open(self.global_config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_config(self, config):
        """ä¿å­˜å…¨å±€é…ç½®æ–‡ä»¶"""
        try:
            with open(self.global_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except:
            pass

    def get_saved_cookie(self):
        """è·å–å·²ä¿å­˜çš„ Cookieï¼ˆç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        encoded = config.get("steam_cookie_encoded", "")
        if encoded:
            try:
                return base64.b64decode(encoded.encode()).decode()
            except:
                pass
        return ""

    def save_cookie(self, cookie_value):
        """ä¿å­˜ Cookieï¼ˆç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        if cookie_value:
            config["steam_cookie_encoded"] = base64.b64encode(cookie_value.encode()).decode()
        else:
            config.pop("steam_cookie_encoded", None)
        self.save_config(config)

    def clear_saved_cookie(self):
        """æ¸…é™¤å·²ä¿å­˜çš„ Cookie"""
        config = self.load_config()
        config.pop("steam_cookie_encoded", None)
        self.save_config(config)

    # ==================== IGDB ç»´åº¦å®šä¹‰ ====================
    IGDB_DIMENSIONS = {
        "genres":              {"endpoint": "/v4/genres",              "game_field": "genres",              "icon": "ğŸ·ï¸", "name": "æ¸¸æˆç±»å‹",  "label": "ğŸ·ï¸ ç±»å‹"},
        "themes":              {"endpoint": "/v4/themes",              "game_field": "themes",              "icon": "ğŸ­", "name": "æ¸¸æˆä¸»é¢˜",  "label": "ğŸ­ ä¸»é¢˜"},
        "keywords":            {"endpoint": "/v4/keywords",            "game_field": "keywords",            "icon": "ğŸ”‘", "name": "å…³é”®è¯",    "label": "ğŸ”‘ å…³é”®è¯"},
        "game_modes":          {"endpoint": "/v4/game_modes",          "game_field": "game_modes",          "icon": "ğŸ®", "name": "æ¸¸æˆæ¨¡å¼",  "label": "ğŸ® æ¨¡å¼"},
        "player_perspectives": {"endpoint": "/v4/player_perspectives", "game_field": "player_perspectives", "icon": "ğŸ‘",  "name": "è§†è§’",      "label": "ğŸ‘ è§†è§’"},
        "franchises":          {"endpoint": "/v4/franchises",          "game_field": "franchises",          "icon": "ğŸ“š", "name": "æ¸¸æˆç³»åˆ—",  "label": "ğŸ“š ç³»åˆ—"},
    }

    # IGDB ç½‘ç«™ URL è·¯å¾„æ˜ å°„ï¼ˆç”¨äºç”Ÿæˆæµè§ˆé“¾æ¥ï¼‰
    IGDB_URL_PATHS = {
        "genres": "genres",
        "themes": "themes",
        "keywords": "categories",
        "game_modes": "game_modes",
        "player_perspectives": "player_perspectives",
        "franchises": "franchises",
    }

    # æ‰€æœ‰éœ€è¦åœ¨ step2 æ‰¹é‡æŸ¥è¯¢çš„ game å­—æ®µï¼ˆé€—å·æ‹¼æ¥ï¼‰
    IGDB_GAME_FIELDS = ",".join(dim["game_field"] for dim in IGDB_DIMENSIONS.values())

    # ==================== IGDB API ç›¸å…³å‡½æ•° ====================
    def get_igdb_credentials(self):
        """è·å–å·²ä¿å­˜çš„ IGDB API å‡­è¯"""
        config = self.load_config()
        client_id = config.get("igdb_client_id", "")
        encoded_secret = config.get("igdb_client_secret_encoded", "")
        client_secret = ""
        if encoded_secret:
            try:
                client_secret = base64.b64decode(encoded_secret.encode()).decode()
            except:
                pass
        return client_id, client_secret

    def save_igdb_credentials(self, client_id, client_secret):
        """ä¿å­˜ IGDB API å‡­è¯ï¼ˆClient Secret ç®€å•æ··æ·†å­˜å‚¨ï¼‰"""
        config = self.load_config()
        config["igdb_client_id"] = client_id
        if client_secret:
            config["igdb_client_secret_encoded"] = base64.b64encode(client_secret.encode()).decode()
        else:
            config.pop("igdb_client_secret_encoded", None)
        self.save_config(config)

    def clear_igdb_credentials(self):
        """æ¸…é™¤ IGDB API å‡­è¯"""
        config = self.load_config()
        config.pop("igdb_client_id", None)
        config.pop("igdb_client_secret_encoded", None)
        config.pop("igdb_access_token", None)
        config.pop("igdb_token_expires_at", None)
        self.save_config(config)

    def get_igdb_access_token(self, force_refresh=False):
        """è·å– IGDB API çš„è®¿é—®ä»¤ç‰Œï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        client_id, client_secret = self.get_igdb_credentials()
        if not client_id or not client_secret:
            return None, "æœªé…ç½® IGDB API å‡­è¯"

        config = self.load_config()
        cached_token = config.get("igdb_access_token", "")
        expires_at = config.get("igdb_token_expires_at", 0)

        # æ£€æŸ¥ç¼“å­˜çš„ä»¤ç‰Œæ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆæå‰ 300 ç§’è¿‡æœŸï¼‰
        current_time = int(time.time())
        if not force_refresh and cached_token and expires_at > current_time + 300:
            return cached_token, None

        # è¯·æ±‚æ–°çš„è®¿é—®ä»¤ç‰Œ
        token_url = f"https://id.twitch.tv/oauth2/token?client_id={client_id}&client_secret={client_secret}&grant_type=client_credentials"

        try:
            req = urllib.request.Request(token_url, method='POST')
            with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as resp:
                data = json.loads(resp.read().decode('utf-8'))

            access_token = data.get("access_token", "")
            expires_in = data.get("expires_in", 0)

            if not access_token:
                return None, "è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥ï¼šå“åº”ä¸­æ—  access_token"

            # ç¼“å­˜ä»¤ç‰Œ
            config["igdb_access_token"] = access_token
            config["igdb_token_expires_at"] = current_time + expires_in
            self.save_config(config)

            return access_token, None

        except urllib.error.HTTPError as e:
            return None, f"HTTP é”™è¯¯ {e.code}ï¼šè·å– IGDB ä»¤ç‰Œå¤±è´¥"
        except urllib.error.URLError as e:
            return None, f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return None, f"è·å–ä»¤ç‰Œå¤±è´¥ï¼š{str(e)}"

    def fetch_igdb_dimension_list(self, dimension, progress_callback=None):
        """è·å– IGDB æŸä¸ªç»´åº¦çš„æ¡ç›®åˆ—è¡¨ï¼ˆåç§°+IDï¼‰

        å¯¹äºå°ç»´åº¦ï¼ˆgenres/themes/game_modes/player_perspectivesï¼‰ï¼šå…¨é‡æ‹‰å–ã€‚
        å¯¹äºå¤§ç»´åº¦ï¼ˆkeywords/franchisesï¼‰ï¼šåªæ‹‰å–æœ¬åœ°ç¼“å­˜ä¸­æœ‰æ•°æ®çš„æ¡ç›®ã€‚

        Args:
            dimension: ç»´åº¦åç§°ï¼Œå¦‚ 'genres', 'themes', 'keywords', ...
            progress_callback: è¿›åº¦å›è°ƒ

        Returns:
            (list_of_items, error): items = [{'id': ..., 'name': ...}, ...]
        """
        dim_info = self.IGDB_DIMENSIONS.get(dimension)
        if not dim_info:
            return [], f"æœªçŸ¥ç»´åº¦: {dimension}"

        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return [], error

        if progress_callback:
            progress_callback(0, 0, f"æ­£åœ¨è·å–{dim_info['name']}åˆ—è¡¨...", "")

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        url = f"https://api.igdb.com{dim_info['endpoint']}"

        # å¤§ç»´åº¦ï¼ˆkeywords/franchisesï¼‰ï¼šåªæŸ¥ç¼“å­˜ä¸­å­˜åœ¨çš„ IDï¼Œé¿å…æ‹‰å–å‡ ä¸‡æ¡æ— ç”¨æ•°æ®
        is_large = dimension in ("keywords", "franchises")

        if is_large:
            cache = self.load_igdb_cache()
            dim_cache = cache.get(dimension, {})
            cached_ids = [k for k in dim_cache.keys() if isinstance(dim_cache.get(k), dict) and "steam_ids" in dim_cache[k]]
            if not cached_ids:
                return [], None

            all_items = []
            batch_size = 500
            total = len(cached_ids)
            for i in range(0, total, batch_size):
                batch = cached_ids[i:i + batch_size]
                ids_str = ",".join(batch)
                body = f"fields id,name,slug; where id = ({ids_str}); limit {batch_size};"
                try:
                    req = urllib.request.Request(url, data=body.encode('utf-8'), headers=headers, method='POST')
                    with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                        batch_items = json.loads(resp.read().decode('utf-8'))
                        all_items.extend(batch_items)
                except urllib.error.HTTPError as e:
                    return [], f"HTTP é”™è¯¯ {e.code}ï¼šè·å–{dim_info['name']}åˆ—è¡¨å¤±è´¥"
                except urllib.error.URLError as e:
                    return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
                except Exception as e:
                    return [], f"è·å–å¤±è´¥ï¼š{str(e)}"

                if progress_callback:
                    progress_callback(min(i + batch_size, total), total,
                                      f"æ­£åœ¨è·å–{dim_info['name']}åç§°...",
                                      f"{len(all_items)}/{total}")
                time.sleep(0.28)
        else:
            # å°ç»´åº¦ï¼šä¸€æ¬¡æ€§å…¨é‡æ‹‰å–
            all_items = []
            offset = 0
            limit = 500
            while True:
                body = f"fields id,name,slug; limit {limit}; offset {offset}; sort name asc;"
                try:
                    req = urllib.request.Request(url, data=body.encode('utf-8'), headers=headers, method='POST')
                    with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                        batch = json.loads(resp.read().decode('utf-8'))
                except urllib.error.HTTPError as e:
                    return [], f"HTTP é”™è¯¯ {e.code}ï¼šè·å–{dim_info['name']}åˆ—è¡¨å¤±è´¥"
                except urllib.error.URLError as e:
                    return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
                except Exception as e:
                    return [], f"è·å–å¤±è´¥ï¼š{str(e)}"

                if not batch:
                    break
                all_items.extend(batch)
                if len(batch) < limit:
                    break
                offset += limit
                time.sleep(0.28)

        all_items.sort(key=lambda x: x.get('name', ''))
        return all_items, None

    def fetch_igdb_genres(self, progress_callback=None):
        """è·å– IGDB æ¸¸æˆç±»å‹åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self.fetch_igdb_dimension_list("genres", progress_callback)

    # ==================== IGDB æœ¬åœ°ç¼“å­˜ ====================

    IGDB_CACHE_EXPIRY_DAYS = 7  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰

    def get_igdb_cache_path(self):
        """è·å– IGDB ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.data_dir, "igdb_cache.json")

    def load_igdb_cache(self):
        """åŠ è½½ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_igdb_cache(self, cache):
        """ä¿å­˜ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False)
        except:
            pass

    def get_igdb_dimension_cache(self, dimension, item_id):
        """è·å–æŸä¸ªç»´åº¦ä¸‹æŸä¸ªæ¡ç›®çš„ç¼“å­˜æ•°æ®

        Args:
            dimension: ç»´åº¦åç§°ï¼Œå¦‚ 'genres', 'themes', 'keywords', ...
            item_id: æ¡ç›® ID

        Returns:
            (steam_ids, cached_at_timestamp) æˆ– (None, None)
        """
        cache = self.load_igdb_cache()
        dim_data = cache.get(dimension, {})
        item_key = str(item_id)
        if item_key in dim_data:
            entry = dim_data[item_key]
            return entry.get("steam_ids", []), entry.get("cached_at", 0)
        return None, None

    def get_igdb_genre_cache(self, genre_id):
        """è·å–æŸä¸ªç±»å‹çš„ç¼“å­˜æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self.get_igdb_dimension_cache("genres", genre_id)

    def set_igdb_dimension_cache(self, dimension, item_id, steam_ids):
        """å†™å…¥æŸä¸ªç»´åº¦ä¸‹æŸä¸ªæ¡ç›®çš„ç¼“å­˜æ•°æ®"""
        cache = self.load_igdb_cache()
        if dimension not in cache:
            cache[dimension] = {}
        cache[dimension][str(item_id)] = {
            "steam_ids": steam_ids,
            "cached_at": time.time(),
        }
        self.save_igdb_cache(cache)

    def set_igdb_genre_cache(self, genre_id, steam_ids):
        """å†™å…¥æŸä¸ªç±»å‹çš„ç¼“å­˜æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        self.set_igdb_dimension_cache("genres", genre_id, steam_ids)

    def is_igdb_cache_valid(self, cached_at):
        """åˆ¤æ–­ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        if not cached_at:
            return False
        age_seconds = time.time() - cached_at
        return age_seconds < self.IGDB_CACHE_EXPIRY_DAYS * 86400

    def get_igdb_dimension_game_counts(self, dimension):
        """è·å–æŸç»´åº¦ä¸‹å„æ¡ç›®çš„ Steam æ¸¸æˆæ•°é‡ï¼ˆä»æœ¬åœ°ç¼“å­˜è¯»å–ï¼‰

        Args:
            dimension: ç»´åº¦åç§°

        Returns:
            dict: {item_id(int): count(int)}ï¼Œæ— ç¼“å­˜åˆ™è¿”å›ç©ºå­—å…¸
        """
        cache = self.load_igdb_cache()
        dim_data = cache.get(dimension, {})
        if not isinstance(dim_data, dict):
            return {}
        result = {}
        for item_key, entry in dim_data.items():
            if isinstance(entry, dict) and "steam_ids" in entry:
                try:
                    result[int(item_key)] = len(entry["steam_ids"])
                except (ValueError, TypeError):
                    pass
        return result

    def get_igdb_cache_summary(self):
        """è·å–ç¼“å­˜æ‘˜è¦ä¿¡æ¯ï¼Œç”¨äº UI æ˜¾ç¤º

        Returns:
            dict: {'dimensions': {dim: {'count': int, 'games': int}}, 'total_steam_games': int,
                   'newest_at': float, 'is_full_dump': bool}
                  å¦‚æœæ— ç¼“å­˜åˆ™è¿”å› None
        """
        cache = self.load_igdb_cache()
        if not cache:
            return None

        meta = cache.get("_meta", {})
        is_full_dump = meta.get("type") == "full_dump"

        # æ–°æ ¼å¼ï¼šæŒ‰ç»´åº¦åˆ†åŒº
        dim_stats = {}
        all_timestamps = []
        total_items = 0

        for dim_name in self.IGDB_DIMENSIONS:
            dim_data = cache.get(dim_name, {})
            if not isinstance(dim_data, dict):
                continue
            count = len(dim_data)
            games = sum(len(entry.get("steam_ids", [])) for entry in dim_data.values() if isinstance(entry, dict))
            timestamps = [entry.get("cached_at", 0) for entry in dim_data.values()
                          if isinstance(entry, dict) and entry.get("cached_at")]
            if count > 0:
                dim_stats[dim_name] = {'count': count, 'games': games}
                total_items += count
                all_timestamps.extend(timestamps)

        # å…¼å®¹æ—§æ ¼å¼ï¼ˆæ— ç»´åº¦åˆ†åŒºï¼Œgenre_id ç›´æ¥åœ¨é¡¶å±‚ï¼‰
        if not dim_stats:
            old_entries = {k: v for k, v in cache.items() if k != "_meta" and isinstance(v, dict) and "steam_ids" in v}
            if old_entries:
                total_genres = len(old_entries)
                total_games = sum(len(entry.get("steam_ids", [])) for entry in old_entries.values())
                timestamps = [entry.get("cached_at", 0) for entry in old_entries.values() if entry.get("cached_at")]
                if not timestamps:
                    return None
                return {
                    'total_genres': total_genres,
                    'total_games': total_games,
                    'oldest_at': min(timestamps),
                    'newest_at': max(timestamps),
                    'is_full_dump': is_full_dump,
                    'total_steam_games': meta.get("total_steam_games", 0),
                    'dimensions': {'genres': {'count': total_genres, 'games': total_games}},
                }

        if not all_timestamps:
            return None

        return {
            'dimensions': dim_stats,
            'total_items': total_items,
            'oldest_at': min(all_timestamps) if all_timestamps else 0,
            'newest_at': max(all_timestamps) if all_timestamps else 0,
            'is_full_dump': is_full_dump,
            'total_steam_games': meta.get("total_steam_games", 0),
            # å‘åå…¼å®¹å­—æ®µ
            'total_genres': dim_stats.get('genres', {}).get('count', 0),
            'total_games': sum(d['games'] for d in dim_stats.values()),
        }

    def clear_igdb_genre_cache(self):
        """æ¸…é™¤æ‰€æœ‰ IGDB ç¼“å­˜"""
        path = self.get_igdb_cache_path()
        if os.path.exists(path):
            try:
                os.remove(path)
            except:
                pass

    # ==================== IGDB API è¯·æ±‚ ====================

    def igdb_api_request(self, url, body, headers):
        """å‘é€ IGDB API è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†é€Ÿç‡é™åˆ¶å’Œé‡è¯•"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                req = urllib.request.Request(url, data=body.encode('utf-8'), headers=headers, method='POST')
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    return json.loads(resp.read().decode('utf-8')), None
            except urllib.error.HTTPError as e:
                if e.code == 429:
                    time.sleep(1.5)
                    continue
                return None, f"HTTP é”™è¯¯ {e.code}"
            except urllib.error.URLError as e:
                return None, f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
            except Exception as e:
                return None, f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
        return None, "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰"

    def build_igdb_full_cache(self, progress_callback=None, cancel_flag=None):
        """ä¸‹è½½ IGDB ä¸­æ‰€æœ‰æœ‰ Steam å…³è”çš„æ¸¸æˆåŠå…¶å¤šç»´åº¦åˆ†ç±»ä¿¡æ¯ï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜ã€‚

        ç­–ç•¥ï¼šå…ˆä» external_games æ‹‰å–æ‰€æœ‰ Steam å…³è”ï¼Œå†æ‰¹é‡æŸ¥ genres/themes/keywords ç­‰ã€‚

        Args:
            progress_callback: fn(current, total, phase_str, detail_str)
            cancel_flag: list[bool]ï¼Œcancel_flag[0]=True æ—¶ä¸­æ­¢

        Returns:
            (genre_map, error): genre_map = {genre_id: [steam_app_ids]}ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œerror = str | None
        """
        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return {}, error

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        # ===== é¢„æŸ¥è¯¢ï¼šè·å– Steam å…³è”è®°å½•çš„æœ€å¤§ IDï¼Œç”¨äºä¼°ç®—è¿›åº¦ =====
        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨ä¼°ç®—æ•°æ®é‡...", "")

        max_ext_id = 0
        body = "fields id; where external_game_source = 1; sort id desc; limit 1;"
        results, err = self.igdb_api_request(
            "https://api.igdb.com/v4/external_games", body, headers)
        if results:
            max_ext_id = results[0].get('id', 0)
        time.sleep(0.28)

        # ===== ç¬¬1æ­¥ï¼šéå† external_games è·å–æ‰€æœ‰ Steam å…³è” =====
        game_to_steam = {}
        last_id = 0
        limit = 500

        while True:
            if cancel_flag and cancel_flag[0]:
                return {}, "ç”¨æˆ·å–æ¶ˆ"

            if progress_callback:
                step1_pct = (last_id / max_ext_id * 50) if max_ext_id > 0 else 0
                progress_callback(int(step1_pct), 100,
                                  "æ­£åœ¨ä¸‹è½½ Steam æ¸¸æˆåˆ—è¡¨...",
                                  f"å·²è·å– {len(game_to_steam)} ä¸ªæ¸¸æˆ")

            body = (f"fields id,uid,game; "
                    f"where external_game_source = 1 & id > {last_id}; "
                    f"sort id asc; limit {limit};")

            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/external_games", body, headers)

            if err:
                return {}, f"ä¸‹è½½ Steam æ¸¸æˆåˆ—è¡¨å¤±è´¥ï¼š{err}"
            if not results:
                break

            for item in results:
                uid = item.get('uid', '')
                game_id = item.get('game')
                ext_id = item.get('id', 0)
                if uid and uid.isdigit() and game_id:
                    game_to_steam[int(game_id)] = int(uid)
                if ext_id > last_id:
                    last_id = ext_id

            if len(results) < limit:
                break
            time.sleep(0.28)

        if not game_to_steam:
            return {}, "æœªæ‰¾åˆ°ä»»ä½• Steam æ¸¸æˆ"

        # ===== ç¬¬2æ­¥ï¼šæ‰¹é‡æŸ¥è¯¢è¿™äº›æ¸¸æˆçš„å¤šç»´åº¦åˆ†ç±»ä¿¡æ¯ =====
        all_game_ids = list(game_to_steam.keys())
        # dim_maps: {dimension: {item_id: set of steam_app_ids}}
        dim_maps = {dim: {} for dim in self.IGDB_DIMENSIONS}
        batch_size = 500
        total_batches = (len(all_game_ids) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            if cancel_flag and cancel_flag[0]:
                return {}, "ç”¨æˆ·å–æ¶ˆ"

            if progress_callback:
                step2_pct = 50 + (batch_idx / total_batches * 50) if total_batches > 0 else 50
                progress_callback(int(step2_pct), 100,
                                  "æ­£åœ¨ä¸‹è½½æ¸¸æˆåˆ†ç±»ä¿¡æ¯...",
                                  f"è¿›åº¦ {batch_idx + 1}/{total_batches}ï¼ˆå…± {len(all_game_ids)} ä¸ªæ¸¸æˆï¼‰")

            batch = all_game_ids[batch_idx * batch_size: (batch_idx + 1) * batch_size]
            ids_str = ",".join(str(gid) for gid in batch)

            body = (f"fields id,{self.IGDB_GAME_FIELDS}; "
                    f"where id = ({ids_str}); "
                    f"limit {limit};")

            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/games", body, headers)

            if err:
                time.sleep(0.28)
                continue

            if results:
                for item in results:
                    gid = item.get('id')
                    if not gid or gid not in game_to_steam:
                        continue
                    steam_id = game_to_steam[gid]
                    # éå†æ¯ä¸ªç»´åº¦
                    for dim_name, dim_info in self.IGDB_DIMENSIONS.items():
                        field_name = dim_info['game_field']
                        item_ids = item.get(field_name, [])
                        if item_ids:
                            for item_id in item_ids:
                                dim_maps[dim_name].setdefault(item_id, set()).add(steam_id)

            time.sleep(0.28)

        # ===== ç¬¬3æ­¥ï¼šå†™å…¥ç¼“å­˜ =====
        cache = {}
        now = time.time()

        for dim_name, dim_data in dim_maps.items():
            cache[dim_name] = {}
            for item_id, steam_ids_set in dim_data.items():
                cache[dim_name][str(item_id)] = {
                    "steam_ids": sorted(steam_ids_set),
                    "cached_at": now,
                }

        # ä¿å­˜ game_to_steam æ˜ å°„ï¼ˆä¾›å…¬å¸æœç´¢ç­‰åŠŸèƒ½ä½¿ç”¨ï¼‰
        cache["_game_to_steam"] = {str(k): v for k, v in game_to_steam.items()}

        dim_summary = ", ".join(f"{self.IGDB_DIMENSIONS[d]['name']} {len(dim_maps[d])}" for d in dim_maps if dim_maps[d])
        cache["_meta"] = {
            "type": "full_dump",
            "cached_at": now,
            "total_steam_games": len(game_to_steam),
            "dimensions": list(self.IGDB_DIMENSIONS.keys()),
        }
        self.save_igdb_cache(cache)

        if progress_callback:
            progress_callback(100, 100,
                              "âœ… ä¸‹è½½å®Œæˆ",
                              f"å…± {len(game_to_steam)} ä¸ª Steam æ¸¸æˆï¼ˆ{dim_summary}ï¼‰")

        # è¿”å›å€¼ä¿æŒ genre_map å½¢å¼ä»¥å…¼å®¹æ—§è°ƒç”¨
        result = {}
        for dim_name, dim_data in dim_maps.items():
            for item_id, sids in dim_data.items():
                if dim_name == "genres":
                    result[item_id] = sorted(sids)
        return result, None

    def fetch_igdb_games_by_dimension(self, dimension, item_id, item_name, progress_callback=None, force_refresh=False):
        """æ ¹æ®ç»´åº¦å’Œæ¡ç›® ID è·å–è¯¥æ¡ç›®ä¸‹æ‰€æœ‰æ¸¸æˆçš„ Steam AppID

        ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å…¨é‡ç¼“å­˜ã€‚å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œåˆ™è‡ªåŠ¨è§¦å‘å…¨é‡æ„å»ºã€‚

        Args:
            dimension: ç»´åº¦åç§°ï¼Œå¦‚ 'genres', 'themes', 'keywords', ...
            item_id: æ¡ç›® ID
            item_name: æ¡ç›®åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        """
        if not force_refresh:
            cached_ids, cached_at = self.get_igdb_dimension_cache(dimension, item_id)
            if cached_ids is not None and self.is_igdb_cache_valid(cached_at):
                if progress_callback:
                    age_hours = (time.time() - cached_at) / 3600
                    progress_callback(len(cached_ids), len(cached_ids),
                                      f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜",
                                      f"{item_name}: {len(cached_ids)} ä¸ªæ¸¸æˆï¼ˆç¼“å­˜äº {age_hours:.0f} å°æ—¶å‰ï¼‰")
                return cached_ids, None

            # è¯¥æ¡ç›®æ— ç¼“å­˜ï¼Œä½†å…¨é‡ç¼“å­˜å¯èƒ½å·²æ„å»ºï¼ˆåªæ˜¯è¯¥æ¡ç›®ç¡®å®æ²¡æœ‰ Steam æ¸¸æˆï¼‰
            cache = self.load_igdb_cache()
            meta = cache.get("_meta", {})
            if meta.get("type") == "full_dump" and self.is_igdb_cache_valid(meta.get("cached_at", 0)):
                if progress_callback:
                    age_hours = (time.time() - meta["cached_at"]) / 3600
                    progress_callback(0, 0,
                                      f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜", f"{item_name}: 0 ä¸ª Steam æ¸¸æˆï¼ˆç¼“å­˜äº {age_hours:.0f} å°æ—¶å‰ï¼‰")
                return [], None

        # === ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼šè§¦å‘ä¸‹è½½ ===
        if progress_callback:
            progress_callback(0, 0, "æœ¬åœ°æ•°æ®ä¸å®Œæ•´ï¼Œæ­£åœ¨ä» IGDB ä¸‹è½½...", "é¦–æ¬¡ä¸‹è½½çº¦éœ€ 5-8 åˆ†é’Ÿ")

        genre_map, error = self.build_igdb_full_cache(progress_callback)
        if error:
            return [], error

        # ä»åˆšæ„å»ºçš„ç¼“å­˜ä¸­è¿”å›ç»“æœ
        cached_ids, _ = self.get_igdb_dimension_cache(dimension, item_id)
        return cached_ids if cached_ids else [], None

    def fetch_igdb_games_by_genre(self, genre_id, genre_name, progress_callback=None, force_refresh=False):
        """æ ¹æ®ç±»å‹ ID è·å–è¯¥ç±»å‹ä¸‹æ‰€æœ‰æ¸¸æˆçš„ Steam AppIDï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self.fetch_igdb_games_by_dimension("genres", genre_id, genre_name, progress_callback, force_refresh)

    # ==================== IGDB å…¬å¸æœç´¢ ====================

    def search_igdb_companies(self, query):
        """æœç´¢ IGDB å…¬å¸ï¼ˆå¼€å‘å•†/å‘è¡Œå•†ï¼‰

        Args:
            query: æœç´¢å…³é”®è¯

        Returns:
            (list_of_companies, error): companies = [{'id': ..., 'name': ..., 'slug': ...}, ...]
        """
        if not query or len(query.strip()) < 2:
            return [], "æœç´¢å…³é”®è¯è‡³å°‘ 2 ä¸ªå­—ç¬¦"

        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return [], error

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        body = f'fields id,name,slug; where name ~ *"{query.strip()}"*; limit 30;'
        results, err = self.igdb_api_request("https://api.igdb.com/v4/companies", body, headers)
        if err:
            return [], err

        return results or [], None

    def count_igdb_company_steam_games(self, company_ids):
        """æ‰¹é‡ç»Ÿè®¡å¤šä¸ªå…¬å¸å„è‡ªå…³è”çš„ Steam æ¸¸æˆæ•°é‡

        ä½¿ç”¨å•æ¬¡æ‰¹é‡æŸ¥è¯¢ involved_companies + æœ¬åœ° game_to_steam ç¼“å­˜ï¼Œ
        é¿å…ä¸ºæ¯ä¸ªå…¬å¸å•ç‹¬å‘èµ· API è¯·æ±‚ã€‚

        Args:
            company_ids: å…¬å¸ ID åˆ—è¡¨

        Returns:
            dict: {company_id: steam_game_count}
        """
        if not company_ids:
            return {}

        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return {}

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰å…¬å¸çš„ involved_companies
        company_games = {cid: set() for cid in company_ids}
        ids_str = ",".join(str(cid) for cid in company_ids)
        offset = 0
        limit = 500

        while True:
            body = (f"fields game,company; "
                    f"where company = ({ids_str}); "
                    f"limit {limit}; offset {offset};")
            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/involved_companies", body, headers)
            if err or not results:
                break
            for item in results:
                cid = item.get('company')
                gid = item.get('game')
                if cid and gid and cid in company_games:
                    company_games[cid].add(int(gid))
            if len(results) < limit:
                break
            offset += limit
            time.sleep(0.28)

        # ç”¨æœ¬åœ°ç¼“å­˜çš„ game_to_steam æ˜ å°„è®¡ç®— Steam æ¸¸æˆæ•°
        cache = self.load_igdb_cache()
        game_to_steam = cache.get("_game_to_steam", {})

        counts = {}
        for cid, game_ids in company_games.items():
            steam_count = sum(1 for gid in game_ids if game_to_steam.get(str(gid)))
            counts[cid] = steam_count

        return counts

    def fetch_igdb_games_by_company(self, company_id, company_name, progress_callback=None):
        """è·å–æŸå…¬å¸å…³è”çš„æ‰€æœ‰ Steam æ¸¸æˆ

        ç­–ç•¥ï¼šæŸ¥ involved_companies â†’ è·å– game IDs â†’ ç”¨æœ¬åœ° game_to_steam æ˜ å°„è½¬æ¢

        Args:
            company_id: IGDB å…¬å¸ ID
            company_name: å…¬å¸åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            progress_callback: è¿›åº¦å›è°ƒ

        Returns:
            (steam_ids, error)
        """
        client_id, _ = self.get_igdb_credentials()
        access_token, error = self.get_igdb_access_token()
        if error:
            return [], error

        headers = {
            'Client-ID': client_id,
            'Authorization': f'Bearer {access_token}',
            'Accept': 'application/json',
        }

        if progress_callback:
            progress_callback(0, 0, f"æ­£åœ¨æŸ¥è¯¢ {company_name} çš„æ¸¸æˆåˆ—è¡¨...", "")

        # æŸ¥è¯¢ involved_companies è·å–è¯¥å…¬å¸å‚ä¸çš„æ‰€æœ‰æ¸¸æˆ
        game_ids = set()
        offset = 0
        limit = 500

        while True:
            body = (f"fields game; "
                    f"where company = {company_id}; "
                    f"limit {limit}; offset {offset};")
            results, err = self.igdb_api_request(
                "https://api.igdb.com/v4/involved_companies", body, headers)

            if err:
                return [], f"æŸ¥è¯¢å…¬å¸æ¸¸æˆå¤±è´¥ï¼š{err}"
            if not results:
                break

            for item in results:
                gid = item.get('game')
                if gid:
                    game_ids.add(int(gid))

            if len(results) < limit:
                break
            offset += limit
            time.sleep(0.28)

        if not game_ids:
            return [], None

        if progress_callback:
            progress_callback(50, 100, f"æ­£åœ¨åŒ¹é… Steam æ¸¸æˆ...", f"å…± {len(game_ids)} ä¸ª IGDB æ¸¸æˆ")

        # å°è¯•ç”¨æœ¬åœ° game_to_steam æ˜ å°„
        cache = self.load_igdb_cache()
        game_to_steam = cache.get("_game_to_steam", {})

        steam_ids = set()
        unmapped_ids = []

        for gid in game_ids:
            steam_id = game_to_steam.get(str(gid))
            if steam_id:
                steam_ids.add(int(steam_id))
            else:
                unmapped_ids.append(gid)

        # å¯¹äºæœªæ˜ å°„çš„æ¸¸æˆï¼Œé€šè¿‡ API æŸ¥è¯¢ external_games
        if unmapped_ids:
            batch_size = 500
            for i in range(0, len(unmapped_ids), batch_size):
                batch = unmapped_ids[i:i + batch_size]
                ids_str = ",".join(str(gid) for gid in batch)
                body = (f"fields uid,game; "
                        f"where external_game_source = 1 & game = ({ids_str}); "
                        f"limit {batch_size};")
                results, err = self.igdb_api_request(
                    "https://api.igdb.com/v4/external_games", body, headers)
                if results:
                    for item in results:
                        uid = item.get('uid', '')
                        if uid and uid.isdigit():
                            steam_ids.add(int(uid))
                time.sleep(0.28)

        if progress_callback:
            progress_callback(100, 100, f"âœ… æŸ¥è¯¢å®Œæˆ",
                              f"{company_name}: {len(steam_ids)} ä¸ª Steam æ¸¸æˆ")

        return sorted(steam_ids), None

    def load_json(self):
        if not self.current_account.storage_path or not os.path.exists(self.current_account.storage_path):
            messagebox.showerror("é”™è¯¯", f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²é€‰æ‹©æœ‰æ•ˆçš„ Steam è´¦å·ã€‚")
            return None
        try:
            with open(self.current_account.storage_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            messagebox.showerror("è¯»å–é”™è¯¯", f"è§£æå¤±è´¥: {e}")
            return None

    def save_json(self, data, create_backup=True, backup_description=""):
        """ä¿å­˜ JSON æ•°æ®åˆ°åŸæ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            create_backup: æ˜¯å¦åœ¨ä¿å­˜å‰åˆ›å»ºå¤‡ä»½
            backup_description: å¤‡ä»½æè¿°
        """
        if not self.current_account.storage_path:
            messagebox.showerror("é”™è¯¯", "æœªé€‰æ‹©è´¦å·ï¼Œæ— æ³•ä¿å­˜ã€‚")
            return False

        # åˆ›å»ºå¤‡ä»½
        if create_backup and self.backup_manager:
            backup_path = self.backup_manager.create_backup(description=backup_description)
            if backup_path:
                backup_info = f"\n\nå·²è‡ªåŠ¨å¤‡ä»½è‡³:\n{os.path.basename(backup_path)}"
            else:
                backup_info = "\n\nâš ï¸ å¤‡ä»½åˆ›å»ºå¤±è´¥"
        else:
            backup_info = ""

        # å†™å…¥åŸæ–‡ä»¶ï¼ˆä½¿ç”¨åŸå­å†™å…¥ï¼‰
        tmp_path = self.current_account.storage_path + ".tmp"
        try:
            with open(tmp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, separators=(',', ':'))

            # åŸå­æ›¿æ¢
            if os.path.exists(self.current_account.storage_path):
                os.replace(tmp_path, self.current_account.storage_path)
            else:
                os.rename(tmp_path, self.current_account.storage_path)

            messagebox.showinfo("æˆåŠŸ", f"æ–‡ä»¶å·²ä¿å­˜ï¼š\n{os.path.basename(self.current_account.storage_path)}{backup_info}")
            return True
        except Exception as e:
            messagebox.showerror("ä¿å­˜å¤±è´¥", f"æ— æ³•å†™å…¥æ–‡ä»¶: {e}")
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except:
                    pass
            return False


    @staticmethod
    def sanitize_filename(name):
        """æ¸…æ´—æ–‡ä»¶åï¼Œæ›¿æ¢ç³»ç»Ÿç¦æ­¢çš„ç‰¹æ®Šå­—ç¬¦"""
        return re.sub(r'[\\/*?:"<>|]', '_', name).strip()

    def get_static_collections(self, data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        return self.get_all_collections_with_refs(data)

    @staticmethod
    def get_all_collections_with_refs(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€æ”¶è—å¤¹ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    icon = "ğŸ”" if is_dynamic else "ğŸ“"
                    collections.append({
                        "entry_ref": entry,
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name"),
                        "added": val_obj.get("added", []),
                        "is_dynamic": is_dynamic,
                        "display_name": f"{icon} {val_obj.get('name', 'æœªå‘½å')}"
                    })
                except Exception:
                    continue
        collections.sort(key=lambda c: (c.get('name') or '').lower())
        return collections

    @staticmethod
    def get_all_collections_ordered(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆæŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¸ Steam å®¢æˆ·ç«¯ä¸€è‡´ï¼‰"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    col_info = {
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name", "æœªå‘½å"),
                        "added": val_obj.get("added", []),
                        "removed": val_obj.get("removed", []),
                        "is_dynamic": is_dynamic
                    }
                    if is_dynamic:
                        col_info["filterSpec"] = val_obj.get("filterSpec")
                    collections.append(col_info)
                except Exception:
                    continue
        collections.sort(key=lambda c: c['name'].lower())
        return collections

    @staticmethod
    def extract_ids_from_html(html_text):
        """æ ¸å¿ƒæå–é€»è¾‘ï¼šä» HTML ä¸­æå– AppID"""
        search_area = html_text
        list_start = html_text.find('id="RecommendationsRows"')
        if list_start == -1:
            list_start = html_text.find('class="creator_grid_ctn"')

        if list_start != -1:
            footer_start = html_text.find('id="footer"', list_start)
            search_area = html_text[list_start: (footer_start if footer_start != -1 else len(html_text))]

        raw_matches = re.findall(r'data-ds-appid="([\d,]+)"', search_area)
        all_ids = []
        for m in raw_matches:
            if ',' in m:
                all_ids.extend(m.split(','))
            else:
                all_ids.append(m)

        return list(dict.fromkeys([int(aid) for aid in all_ids if aid.isdigit()]))

    def extract_page_name_from_html(self, html_text, url_hint=""):
        """ä» HTML ä¸­æ™ºèƒ½æå–é¡µé¢åç§°ï¼ˆå¸¦ç±»å‹å‰ç¼€ï¼‰"""
        type_name_cn = "åˆ—è¡¨"
        if url_hint:
            page_type, _ = self.extract_steam_list_info(url_hint)
            type_names = {
                "curator": "é‰´èµå®¶",
                "publisher": "å‘è¡Œå•†",
                "developer": "å¼€å‘å•†",
                "franchise": "ç³»åˆ—",
                "genre": "ç±»å‹",
                "category": "åˆ†ç±»",
            }
            type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        if "curator" in html_text.lower() or "é‰´èµå®¶" in html_text:
            type_name_cn = "é‰´èµå®¶"
        elif "publisher" in html_text.lower():
            type_name_cn = "å‘è¡Œå•†"
        elif "developer" in html_text.lower():
            type_name_cn = "å¼€å‘å•†"

        name = None
        match = re.search(r'class="curator_name".*?><a.*?>(.*?)</a>', html_text, re.S)
        if match:
            name = match.group(1).strip()

        if not name:
            match = re.search(r'<title>(.*?)</title>', html_text, re.I)
            if match:
                title = match.group(1)
                title = re.sub(r'\s*[-â€“â€”]\s*Steam.*$', '', title, flags=re.I)
                title = re.sub(r'\s*on Steam.*$', '', title, flags=re.I)
                title = re.sub(r'^Steam é‰´èµå®¶ï¼š', '', title)
                title = re.sub(r'^Steam Curator:\s*', '', title, flags=re.I)
                name = title.strip()

        if name:
            return f"{type_name_cn}ï¼š{name}"
        return f"{type_name_cn}ï¼šæœªçŸ¥"

    def extract_curator_name(self, html_text):
        """ä» HTML ä¸­æ™ºèƒ½æå–é‰´èµå®¶åç§°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self.extract_page_name_from_html(html_text)

    @staticmethod
    def extract_steam_list_info(url_or_id):
        """ä» URL æˆ–ç›´æ¥è¾“å…¥ä¸­æå– Steam åˆ—è¡¨é¡µé¢ä¿¡æ¯"""
        text = url_or_id.strip()

        if text.isdigit():
            return "curator", text

        patterns = [
            (r'/curator/(\d+)', "curator"),
            (r'/publisher/([^/?#]+)', "publisher"),
            (r'/developer/([^/?#]+)', "developer"),
            (r'/franchise/([^/?#]+)', "franchise"),
            (r'/genre/([^/?#]+)', "genre"),
            (r'/category/([^/?#]+)', "category"),
        ]

        for pattern, page_type in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return page_type, match.group(1)

        return None, None

    def fetch_steam_list(self, page_type, identifier, progress_callback=None, login_cookies=None):
        """é€šè¿‡ Steam API è‡ªåŠ¨è·å–åˆ—è¡¨é¡µé¢çš„æ‰€æœ‰æ¸¸æˆ"""
        type_names = {
            "curator": "é‰´èµå®¶",
            "publisher": "å‘è¡Œå•†",
            "developer": "å¼€å‘å•†",
            "franchise": "ç³»åˆ—",
            "genre": "ç±»å‹",
            "category": "åˆ†ç±»",
        }
        type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        base_cookies = "birthtime=283993201; wants_mature_content=1; mature_content=1; lastagecheckage=1-0-1979; steamCountry=US%7C0"
        has_login = login_cookies is not None and len(login_cookies.strip()) > 0

        if has_login:
            cookies = f"{login_cookies}; {base_cookies}"
        else:
            cookies = base_cookies

        if page_type in ("curator", "publisher", "developer"):
            return self.fetch_curator_style_api(page_type, identifier, type_name_cn, cookies, has_login,
                                                 progress_callback)
        else:
            return self.fetch_generic_list(page_type, identifier, type_name_cn, cookies, has_login, progress_callback)

    def fetch_curator_style_api(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """ç»Ÿä¸€çš„ ajaxgetfilteredrecommendations API æŠ“å–"""
        from urllib.parse import unquote

        page_url = f"https://store.steampowered.com/{page_type}/{identifier}/"
        curator_id = None
        page_name = None

        headers_html = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cookie': cookies,
        }

        if page_type == "curator":
            curator_id = identifier
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨éªŒè¯é‰´èµå®¶é¡µé¢...", "æ­£åœ¨è¿æ¥ Steam å•†åº—...")
            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')

                name_patterns = [
                    r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                    r'<title>Steam é‰´èµå®¶ï¼š([^<]+?)</title>',
                    r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
                ]
                for pattern in name_patterns:
                    match = re.search(pattern, html_content, re.S | re.I)
                    if match:
                        extracted = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                        extracted = extracted.replace('&amp;', '&').replace('&quot;', '"')
                        if extracted and len(extracted) < 100:
                            page_name = extracted
                            break

            except urllib.error.HTTPError:
                pass
            except Exception:
                pass
        else:
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢ä¿¡æ¯...", f"æ­£åœ¨è®¿é—® {page_type}/{identifier} ...")

            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')

                clanid_match = re.search(r'curator_clanid[=:][\s"\']*(\d+)', html_content)
                if clanid_match:
                    curator_id = clanid_match.group(1)

                name_patterns = [
                    r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                    r'<title>(?:Steam (?:Publisher|Developer):\s*)?([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
                ]
                for pattern in name_patterns:
                    match = re.search(pattern, html_content, re.S | re.I)
                    if match:
                        extracted = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                        extracted = extracted.replace('&amp;', '&').replace('&quot;', '"')
                        if extracted and len(extracted) < 100:
                            page_name = extracted
                            break

            except urllib.error.HTTPError as e:
                return [], None, f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—®è¯¥{type_name_cn}é¡µé¢ã€‚", has_login
            except Exception as e:
                return [], None, f"è·å–é¡µé¢å¤±è´¥ï¼š{str(e)}", has_login

        if not curator_id:
            return [], None, f"æ— æ³•ä»è¯¥{type_name_cn}é¡µé¢æå– curator IDã€‚", has_login

        base_url = f"https://store.steampowered.com/curator/{curator_id}/ajaxgetfilteredrecommendations/"

        lang_configs = [
            ("schinese", "zh-CN,zh;q=0.9,en;q=0.8", "ç®€ä½“ä¸­æ–‡"),
            ("english", "en-US,en;q=0.9", "English"),
            ("japanese", "ja,en;q=0.8", "æ—¥æœ¬èª"),
            ("tchinese", "zh-TW,zh;q=0.9,en;q=0.8", "ç¹é«”ä¸­æ–‡"),
            ("koreana", "ko,en;q=0.8", "í•œêµ­ì–´"),
        ]

        all_unique_ids = set()
        max_total = 0

        for lang_idx, (lang_code, accept_lang, lang_display) in enumerate(lang_configs):
            headers_api = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': accept_lang,
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': page_url,
                'Cookie': cookies,
            }

            start = 0
            count = 100
            total_count = None
            lang_page = 0

            if progress_callback:
                progress_callback(
                    len(all_unique_ids), max_total,
                    f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                    f"ğŸŒ æ‰«æè¯­è¨€ [{lang_idx + 1}/{len(lang_configs)}]ï¼š{lang_display} â€” æ­£åœ¨è¿æ¥..."
                )

            while True:
                url = f"{base_url}?start={start}&count={count}&l={lang_code}"
                lang_page += 1

                try:
                    req = urllib.request.Request(url, headers=headers_api)
                    with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                        data = json.loads(resp.read().decode('utf-8'))

                    if not data.get('success'):
                        break

                    if total_count is None:
                        total_count = int(data.get('total_count', 0))
                        if total_count == 0:
                            break
                        if total_count > max_total:
                            max_total = total_count

                    html_chunk = data.get('results_html', '')
                    new_in_page = 0
                    if html_chunk:
                        chunk_ids = re.findall(r'data-ds-appid="(\d+)"', html_chunk)
                        for aid in chunk_ids:
                            aid_int = int(aid)
                            if aid_int not in all_unique_ids:
                                new_in_page += 1
                            all_unique_ids.add(aid_int)

                        if page_name is None:
                            name_match = re.search(r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>', html_chunk, re.S)
                            if name_match:
                                page_name = re.sub(r'<[^>]+>', '', name_match.group(1)).strip()

                    if progress_callback:
                        total_pages = (total_count + count - 1) // count if total_count else "?"
                        progress_callback(
                            len(all_unique_ids), max_total,
                            f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                            f"ğŸŒ [{lang_idx + 1}/{len(lang_configs)}] {lang_display} â€” ç¬¬ {lang_page}/{total_pages} é¡µï¼ˆæœ¬é¡µæ–°å¢ {new_in_page}ï¼Œå…± {len(chunk_ids) if html_chunk else 0} æ¡ï¼‰"
                        )

                    start += count
                    if start >= total_count or not html_chunk:
                        break

                    time.sleep(0.1)

                except Exception:
                    break

            if progress_callback:
                progress_callback(
                    len(all_unique_ids), max_total if max_total else len(all_unique_ids),
                    f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                    f"âœ… {lang_display} æ‰«æå®Œæˆ â€” å½“å‰å…± {len(all_unique_ids)} ä¸ªå”¯ä¸€æ¸¸æˆ"
                )

            time.sleep(0.2)

        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}æ²¡æœ‰ä»»ä½•æ¸¸æˆï¼Œæˆ–æ ‡è¯†ç¬¦æ— æ•ˆã€‚\nè¯·æ£€æŸ¥ URL æ˜¯å¦æ­£ç¡®ã€‚", has_login

        unique_ids = list(all_unique_ids)

        if page_name:
            display_name = f"{type_name_cn}ï¼š{page_name}"
        else:
            display_name = f"{type_name_cn}ï¼š{unquote(identifier)}"

        return unique_ids, display_name, None, has_login

    def fetch_generic_list(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """é€šè¿‡é€šç”¨æ–¹å¼æŠ“å–å‘è¡Œå•†/å¼€å‘å•†/ç³»åˆ—ç­‰é¡µé¢çš„æ¸¸æˆåˆ—è¡¨"""
        from urllib.parse import unquote

        base_url = f"https://store.steampowered.com/{page_type}/{identifier}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cookie': cookies,
        }

        all_unique_ids = set()
        page_name = None

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢...", f"æ­£åœ¨è¿æ¥ {page_type}/{identifier} ...")

        try:
            req = urllib.request.Request(base_url, headers=headers)
            with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            name_patterns = [
                r'<div class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                r'<div class="page_title_area[^"]*"[^>]*>.*?<span[^>]*>(.*?)</span>',
                r'<h2 class="pageheader">(.*?)</h2>',
                r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam|\s*on Steam)?</title>',
            ]

            for pattern in name_patterns:
                match = re.search(pattern, html_content, re.S | re.I)
                if match:
                    extracted_name = match.group(1).strip()
                    extracted_name = re.sub(r'<[^>]+>', '', extracted_name)
                    extracted_name = extracted_name.replace('&amp;', '&').replace('&quot;', '"')
                    if extracted_name and len(extracted_name) < 100:
                        page_name = extracted_name
                        break

            if not page_name:
                page_name = unquote(identifier).replace('%20', ' ').replace('+', ' ')

            ids = self.extract_ids_from_html(html_content)
            for aid in ids:
                all_unique_ids.add(aid)

            if progress_callback:
                progress_callback(len(all_unique_ids), len(all_unique_ids), "å·²è·å–ä¸»é¡µé¢",
                                  f"ğŸ“„ ä¸»é¡µé¢æå–äº† {len(ids)} ä¸ªæ¸¸æˆï¼Œæ­£åœ¨æ£€æŸ¥åˆ†é¡µ...")

            page = 2
            while True:
                ajax_url = f"{base_url}?page={page}"
                try:
                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"æ­£åœ¨è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ æ­£åœ¨åŠ è½½ç¬¬ {page} é¡µ...")

                    req = urllib.request.Request(ajax_url, headers=headers)
                    with urllib.request.urlopen(req, timeout=15, context=self.ssl_context) as resp:
                        page_html = resp.read().decode('utf-8')

                    page_ids = self.extract_ids_from_html(page_html)
                    if not page_ids or all(aid in all_unique_ids for aid in page_ids):
                        break

                    new_count = sum(1 for aid in page_ids if aid not in all_unique_ids)
                    for aid in page_ids:
                        all_unique_ids.add(aid)

                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"å·²è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ ç¬¬ {page} é¡µæ–°å¢ {new_count} ä¸ªæ¸¸æˆï¼Œå½“å‰å…± {len(all_unique_ids)} ä¸ª")

                    page += 1
                    time.sleep(0.3)

                    if page > 50:
                        break

                except Exception:
                    break

        except urllib.error.HTTPError as e:
            return [], None, f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—®è¯¥é¡µé¢ã€‚", has_login
        except Exception as e:
            return [], None, f"è·å–å¤±è´¥ï¼š{str(e)}", has_login

        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}é¡µé¢æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¸¸æˆã€‚", has_login

        unique_ids = list(all_unique_ids)
        display_name = f"{type_name_cn}ï¼š{page_name}"

        return unique_ids, display_name, None, has_login

    @staticmethod
    def extract_ids_from_steamdb_html(html_text):
        """ä» SteamDB é¡µé¢æºä»£ç ä¸­æå– AppID"""
        tbody_match = re.search(r'<tbody.*?>(.*?)</tbody>', html_text, re.DOTALL)
        if not tbody_match:
            return []
        return [int(aid) for aid in re.findall(r'data-appid="(\d+)"', tbody_match.group(1))]

    def perform_incremental_update(self, data, target_entry, new_ids_from_src, raw_name):
        """æ ¸å¿ƒå¢é‡æ›´æ–°é€»è¾‘ï¼šä¸»æ”¶è—å¤¹è¿½åŠ  + ç”Ÿæˆä¸¤ä¸ªå·®å¼‚å¤‡ä»½æ–‡ä»¶å¤¹

        Returns:
            (added_count, removed_count, total_count, is_updated)
            å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œis_updated ä¸º Falseï¼Œæ­¤æ—¶ä¸ä¼šåšä»»ä½•ä¿®æ”¹
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_ids = val_obj.get("added", [])

        old_set = set(old_ids)
        src_set = set(new_ids_from_src)

        added_list = [aid for aid in new_ids_from_src if aid not in old_set]
        removed_list = [aid for aid in old_ids if aid not in src_set]

        # å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œä¸åšä»»ä½•æ“ä½œ
        if not added_list:
            return 0, len(removed_list), len(old_ids), False

        # æœ‰æ–°å¢ï¼Œæ‰§è¡Œæ›´æ–°
        val_obj['added'] = old_ids + added_list
        clean_name = raw_name.replace(self.induce_suffix, "").strip()
        val_obj['name'] = f"{clean_name}{self.induce_suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        # åˆ›å»ºè¾…åŠ©æ”¶è—å¤¹
        self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå¤šçš„", added_list)
        if removed_list:
            self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå°‘çš„", removed_list)

        return len(added_list), len(removed_list), len(val_obj['added']), True

    def perform_replace_update(self, data, target_entry, new_ids):
        """æ›¿æ¢å¼æ›´æ–°ï¼šç›´æ¥ç”¨æ–° ID åˆ—è¡¨æ›¿æ¢ç›®æ ‡æ”¶è—å¤¹çš„å†…å®¹

        Returns:
            (old_count, new_count)
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_count = len(val_obj.get("added", []))

        val_obj['added'] = new_ids
        clean_name = val_obj.get('name', '').replace(self.induce_suffix, "").strip()
        val_obj['name'] = f"{clean_name}{self.induce_suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        return old_count, len(new_ids)

    # --- æ”¶è—å¤¹å¯¼å‡º/å¯¼å…¥ï¼ˆä¸¤ç§æ ¼å¼ï¼‰ ---

    @staticmethod
    def export_collections_appid_list(collections):
        """æ ¼å¼ä¸€ï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å»é‡ AppID åˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰
        åŠ¨æ€æ”¶è—å¤¹åªå¯¼å‡ºå…¶ added åˆ—è¡¨ã€‚"""
        seen = set()
        unique_ids = []
        for col in collections:
            for aid in col.get('added', []):
                if aid not in seen:
                    seen.add(aid)
                    unique_ids.append(aid)
        return unique_ids

    @staticmethod
    def export_collections_structured(collections):
        """æ ¼å¼äºŒï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å®Œæ•´ç»“æ„åŒ– JSON
        åŒ…å«åç§°ã€ç±»å‹ã€appidã€åŠ¨æ€é€»è¾‘ç­‰ã€‚"""
        export_data = {
            "format": "steam_collections_structured",
            "version": 1,
            "exported_at": datetime.now().isoformat(),
            "collections": []
        }
        for col in collections:
            entry = {
                "name": col.get("name", "æœªå‘½å"),
                "is_dynamic": col.get("is_dynamic", False),
                "added": col.get("added", []),
                "removed": col.get("removed", []),
            }
            if col.get("is_dynamic") and col.get("filterSpec"):
                entry["filterSpec"] = col["filterSpec"]
            export_data["collections"].append(entry)
        return export_data

    def import_collections_appid_list(self, file_path, data):
        """æ ¼å¼ä¸€ï¼šå¯¼å…¥ä¸€è¡Œä¸€ä¸ª AppID çš„åˆ—è¡¨æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªæ–°æ”¶è—å¤¹"""
        file_title = os.path.splitext(os.path.basename(file_path))[0]
        with open(file_path, 'r', encoding='utf-8') as f:
            app_ids = [int(line.strip()) for line in f if line.strip().isdigit()]
        if not app_ids:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ AppIDã€‚"
        self.add_static_collection(data, file_title, app_ids)
        return len(app_ids), None

    def import_collections_structured(self, file_path, data):
        """æ ¼å¼äºŒï¼šå¯¼å…¥ç»“æ„åŒ– JSON æ–‡ä»¶ï¼Œè¿˜åŸå¤šä¸ªæ”¶è—å¤¹ï¼ˆå«åŠ¨æ€é€»è¾‘ï¼‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
        except JSONDecodeError as e:
            return None, "æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"

        if import_data.get("format") != "steam_collections_structured":
            return None, "æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…ï¼šç¼ºå°‘ format æ ‡è¯†ã€‚"

        imported_cols = import_data.get("collections", [])
        if not imported_cols:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æ”¶è—å¤¹æ•°æ®ã€‚"

        count = 0
        for col in imported_cols:
            name = col.get("name", "å¯¼å…¥çš„æ”¶è—å¤¹")
            is_dynamic = col.get("is_dynamic", False)
            added = col.get("added", [])
            removed = col.get("removed", [])

            if is_dynamic and "filterSpec" in col:
                # è¿˜åŸåŠ¨æ€æ”¶è—å¤¹
                col_id = f"uc-{secrets.token_hex(4)}"
                storage_key = f"user-collections.{col_id}"
                val_obj = {
                    "id": col_id,
                    "name": name + self.induce_suffix,
                    "added": added,
                    "removed": removed,
                    "filterSpec": col["filterSpec"]
                }
                new_entry = [storage_key, {
                    "key": storage_key,
                    "timestamp": int(time.time()),
                    "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                    "version": self.next_version(data),
                    "conflictResolutionMethod": "custom",
                    "strMethodId": "union-collections"
                }]
                data.append(new_entry)
            else:
                # é™æ€æ”¶è—å¤¹
                self.add_static_collection(data, name.replace(self.induce_suffix, "").strip(), added)
            count += 1

        return count, None

    def add_dynamic_collection(self, data, name, friend_code):
        col_id = f"uc-{secrets.token_hex(4)}"
        storage_key = f"user-collections.{col_id}"
        filter_groups = [{"rgOptions": [], "bAcceptUnion": False} for _ in range(9)]
        filter_groups[0]["bAcceptUnion"] = True
        filter_groups[6]["rgOptions"] = [int(friend_code)]
        val_obj = {"id": col_id, "name": name + self.induce_suffix, "added": [], "removed": [],
                   "filterSpec": {"nFormatVersion": 2, "strSearchText": "", "filterGroups": filter_groups,
                                  "setSuggestions": {}}}
        new_entry = [storage_key, {"key": storage_key, "timestamp": int(time.time()),
                                   "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                                   "version": self.next_version(data),
                                   "conflictResolutionMethod": "custom", "strMethodId": "union-collections"}]
        data.append(new_entry)

    def fetch_steam250_ids(self, url, progress_callback=None):
        """ä» Steam250 é¡µé¢æå– AppID åˆ—è¡¨"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        }

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è¿æ¥ Steam250...", "")

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=20, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è§£æé¡µé¢...", "")

            raw_ids = re.findall(r'store\.steampowered\.com/app/(\d+)', html_content)

            unique_ids = []
            for aid in raw_ids:
                if aid not in unique_ids:
                    unique_ids.append(aid)

            app_ids = [int(aid) for aid in unique_ids[:250]]

            if not app_ids:
                return [], "æœªèƒ½ä»é¡µé¢æå–åˆ°ä»»ä½• AppIDã€‚é¡µé¢ç»“æ„å¯èƒ½å·²å˜åŒ–ã€‚"

            return app_ids, None

        except urllib.error.HTTPError as e:
            return [], f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—® Steam250ã€‚"
        except urllib.error.URLError as e:
            return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return [], f"æå–å¤±è´¥ï¼š{str(e)}"


